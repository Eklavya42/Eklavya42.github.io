<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="map[name:Eklavya Chopra]">
<meta name="description" content="What is Histogram of Oriented Gradients (HOG)? Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005. Histogram of Oriented Gradients (HOG) is a feature descriptor used in image processing, mainly for object detection. A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information from it.
The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions." />
<meta name="keywords" content=", face detection, machine learning, deep learning" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="/posts/2019/03/face-detection-using-histogram-of-oriented-gradients/" />

<title>Eklavya Chopra</title>


<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.fa08e087a93535b739f7181f05cc10b1b0fddfba0a5a2e0c8d2db8effb14a9fe.css">





    <link rel='icon' href="https://img.icons8.com/dusk/64/000000/sigma.png" type='image/x-icon'>
    
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">



<meta itemprop="name" content="Face Detection using Histogram of Oriented Gradients">
<meta itemprop="description" content="What is Histogram of Oriented Gradients (HOG)? Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005. Histogram of Oriented Gradients (HOG) is a feature descriptor used in image processing, mainly for object detection. A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information from it.
The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions.">
<meta itemprop="datePublished" content="2019-03-11T17:15:12&#43;05:30" />
<meta itemprop="dateModified" content="2019-03-11T17:15:12&#43;05:30" />
<meta itemprop="wordCount" content="1334">
<meta itemprop="image" content=""/>



<meta itemprop="keywords" content="face detection,machine learning,deep learning," />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content=""/>

<meta name="twitter:title" content="Face Detection using Histogram of Oriented Gradients"/>
<meta name="twitter:description" content="What is Histogram of Oriented Gradients (HOG)? Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005. Histogram of Oriented Gradients (HOG) is a feature descriptor used in image processing, mainly for object detection. A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information from it.
The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions."/>



    <meta property="og:title" content="Face Detection using Histogram of Oriented Gradients" />
<meta property="og:description" content="What is Histogram of Oriented Gradients (HOG)? Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005. Histogram of Oriented Gradients (HOG) is a feature descriptor used in image processing, mainly for object detection. A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information from it.
The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/2019/03/face-detection-using-histogram-of-oriented-gradients/" />
<meta property="og:image" content=""/>
<meta property="article:published_time" content="2019-03-11T17:15:12+05:30" />
<meta property="article:modified_time" content="2019-03-11T17:15:12+05:30" />





    <meta property="article:section" content="machine learning" />



    <meta property="article:published_time" content="2019-03-11 17:15:12 &#43;0530 IST" />









<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
.btn {
  background-color:  #ff1a75;
  border: none;
  color: white;
  padding: 12px 30px;
  cursor: pointer;
  font-size: 20px;
  margin: 0 auto;

}

 
.btn:hover {
  background-color:  #b30047;
}
</style>

    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner">
      <li><a href="https://eklavya42.github.io/about/">About</a></li>
      <li><a href="https://eklavya42.github.io/posts/">Posts</a></li>
      <li><a href="https://eklavya42.github.io/resume/">Resume</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>7 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="/posts/2019/03/face-detection-using-histogram-of-oriented-gradients/">Face Detection using Histogram of Oriented Gradients</a>
            </h1>

            

            <div class="post-content">
                <h2 id="what-is-histogram-of-oriented-gradients-hog">What is Histogram of Oriented Gradients (HOG)?</h2>
<p>Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005. Histogram of Oriented Gradients (HOG) is a feature descriptor used in image processing, mainly for object detection. A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information from it.</p>
<p>The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions. The x and y derivatives of an image (Gradients) are useful because the magnitude of gradients is large around edges and corners due to abrupt change in intensity and we know that edges and corners pack in a lot more information about object shape than flat regions. So, the histograms of directions of gradients are used as features in this descriptor.</p>
<h2 id="workflow-of-face-detection-using-hog">Workflow of face detection using HOG</h2>
<p>Now that we know basic principle of Histogram of Oriented Gradients we will be moving into how we calculate the histograms and how these feature vectors, that are obtained from the HOG descriptor, are used by the classifier such a SVM to detect the concerned object.</p>

    <img src="https://i.imgur.com/5qac3GF.png"  class="center"  />


<center>  <figcaption>Steps for Object Detection with HOG</figcaption> </center>
<h2 id="how-histogram-of-oriented-gradientshog-works">How Histogram of Oriented Gradients(HOG) Works?</h2>
<h3 id="pre-processing">Pre-processing</h3>
<p>Preprocessing of image involves normalising the image but it is entirely optional. It is used to improve performance of the HOG descriptor. Since, here we are building a simple descriptor we don&rsquo;t use any normalisation in preprocessing.</p>
<h3 id="computing-gradient">Computing Gradient</h3>
<p>The first actual step in the HOG descriptor is to compute the image gradient in both the x and y direction.</p>
<p>Let us take an example. Say the pixel Q has values surrounding it as shown below:</p>

    <img src="https://i.imgur.com/sdA8be8.png"  class="center"  />


<p>We can calculate the Gradient magnitude for Q in x and y direction as follow:
$$
G_x = 100 -50 =50
$$</p>
<p>$$
G_y = 120 -70 =50
$$</p>
<p>We can get the magnitude of the gradient as:</p>
<p>$$
G= \sqrt{(G_x)^2 + (G_y)^2} = 70.7
$$
And the direction of the gradient as :</p>
<p>$$
\theta = arctan({\frac {G_y} {G_x}}) = 45^\circ
$$</p>
<h3 id="compute-histogram-of-gradients-in-88-cells">Compute Histogram of Gradients in 8×8 cells</h3>
<ul>
<li>
<p>The image is divided into 8×8 cell blocks and a histogram of gradients is calculated for each 8×8 cell block.</p>
</li>
<li>
<p>The histogram is essentially a vector of 9 buckets ( numbers ) corresponding to angles from \(0^ \circ\) to  \(180^ \circ\) (\(20^ \circ\) increments.)</p>
</li>
<li>
<p>The values of these 64 cells (8X8) are binned and cumulatively added into these 9 buckets.</p>
</li>
<li>
<p>This essentially reduces 64 values into 9 values.</p>
</li>
</ul>
<p>A great illustration of this is shown on  <a href="https://www.learnopencv.com/histogram-of-oriented-gradients/">learnopencv</a>. The following figure shows how it is done. The blue pixel encircled has an angle of \(80^ \circ\) and magnitude of 2. So it adds 2 to the 5th bin. The gradient at the pixel encircled using red has an angle of \(10^ \circ\) and magnitude of 4. Since \(0^ \circ\) is half way between \(0^ \circ\) and \(20^ \circ\), the vote by the pixel splits evenly into the two bins.</p>

    <img src="https://i.imgur.com/6zN14kq.png"  class="center"  />


<center>  <figcaption>Illustration of splitting of gradient magnitude according to gradient direction (Image source: https://www.learnopencv.com/histogram-of-oriented-gradients)</figcaption> </center>
<h3 id="block-normalisation">Block Normalisation</h3>
<p>After the creation of histogram of oriented gradients we need to something else too. Gradient is sensitive to overall lighting. If we say divide/multiply pixel values by some constant in order to make it lighter/ darker the gradient magnitude will change and so will histogram values. We want that histogram values be independent of lighting. Normalisation is done on the histogram vector v within a block. One of the following norms could be used:</p>
<ul>
<li>L1 norm</li>
<li>L2 norm</li>
<li>L2-Hys(Lowe-style clipped L2 norm)</li>
</ul>
<p>Now, we could simply normalise the 9×1 histogram vector but it is better to normalise a bigger sized block of 16×16. A 16×16 block has 4 histograms (8×8 cell results to one histogram) which can be concatenated to form a 36 x 1 element vector and normalised. The 16×16 window then moves by 8 pixels and a normalised 36×1 vector is calculated over this window and the process is repeated for the image.</p>
<h3 id="calculate-hog-descriptor-vector">Calculate HOG Descriptor vector</h3>
<ul>
<li>To calculate the final feature vector for the entire image patch, the 36×1 vectors are concatenated into one giant vector.</li>
<li>So, say if there was an input picture of size 64×64 then the 16×16 block has 7 positions horizontally and 7 position vertically.</li>
<li>In one 16×16 block we have 4 histograms which after normalisation concatenate to form a 36×1 vector.</li>
<li>This block moves 7 positions horizontally and vertically totalling it to 7×7 = 49 positions.</li>
<li>So when we concatenate them all into one giant vector we obtain a 36×49 = 1764 dimensional vector.</li>
</ul>
<p>This vector is now used to train classifiers such as SVM and then do object detection.</p>
<h3 id="visualization-of-hog-features">Visualization of HOG features</h3>
<p>Here is a snippet to visualise HOG features of an Image provided in  <a href="https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html">Scikit-Image&rsquo;s docs</a>  to visualize HOG features.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#f92672">from</span> skimage.feature <span style="color:#f92672">import</span> hog
<span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> data, exposure


image <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>astronaut()

fd, hog_image <span style="color:#f92672">=</span> hog(image, orientations<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, pixels_per_cell<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>),
                    cells_per_block<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), visualize<span style="color:#f92672">=</span>True, multichannel<span style="color:#f92672">=</span>True)

fig, (ax1, ax2) <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>), sharex<span style="color:#f92672">=</span>True, sharey<span style="color:#f92672">=</span>True)

ax1<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
ax1<span style="color:#f92672">.</span>imshow(image, cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>gray)
ax1<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Input image&#39;</span>)

<span style="color:#75715e"># Rescale histogram for better display</span>
hog_image_rescaled <span style="color:#f92672">=</span> exposure<span style="color:#f92672">.</span>rescale_intensity(hog_image, in_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>))

ax2<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
ax2<span style="color:#f92672">.</span>imshow(hog_image_rescaled, cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>gray)
ax2<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Histogram of Oriented Gradients&#39;</span>)
plt<span style="color:#f92672">.</span>show()


</code></pre></div><p>Output as follow :</p>

    <img src="https://i.imgur.com/EnNkv49.png"  class="center"  />


<center>  <figcaption>Visualisation of HOG features of image of the astronaut Eileen Collins.</figcaption> </center>
<h2 id="implementation">Implementation</h2>
<p>In this tutorial we will be performing a simple Face Detection using HOG features.<br>
We need to first train the classifier in order to do face detection so first we will need to have training set for the classifier.</p>
<h3 id="training-set">Training Set</h3>
<ul>
<li>Positive training samples</li>
</ul>
<p>Labelled Faces in the Wild dataset provided by Scikit-Learn consists of variety of faces which is perfect for our positive set.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> fetch_lfw_people
faces <span style="color:#f92672">=</span> fetch_lfw_people()
positive_patches <span style="color:#f92672">=</span> faces<span style="color:#f92672">.</span>images

</code></pre></div><ul>
<li>Negative training samples</li>
</ul>
<p>For Negative set we need images without face on them. Scikit-Image offers images which can be used in this case. To increase the size of negative set we extract patches of image at different scale using Patch Extractor from Scikit-Learn.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> data, transform
<span style="color:#f92672">from</span> sklearn.feature_extraction.image <span style="color:#f92672">import</span> PatchExtractor

imgs_to_use <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;camera&#39;</span>, <span style="color:#e6db74">&#39;text&#39;</span>, <span style="color:#e6db74">&#39;coins&#39;</span>, <span style="color:#e6db74">&#39;moon&#39;</span>,
               <span style="color:#e6db74">&#39;page&#39;</span>, <span style="color:#e6db74">&#39;clock&#39;</span>, <span style="color:#e6db74">&#39;immunohistochemistry&#39;</span>,
               <span style="color:#e6db74">&#39;chelsea&#39;</span>, <span style="color:#e6db74">&#39;coffee&#39;</span>, <span style="color:#e6db74">&#39;hubble_deep_field&#39;</span>]
images <span style="color:#f92672">=</span> [color<span style="color:#f92672">.</span>rgb2gray(getattr(data, name)())
          <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> imgs_to_use]


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_patches</span>(img, N, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, patch_size<span style="color:#f92672">=</span>positive_patches[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>shape):
    extracted_patch_size <span style="color:#f92672">=</span> tuple((scale <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>array(patch_size))<span style="color:#f92672">.</span>astype(int))
    extractor <span style="color:#f92672">=</span> PatchExtractor(patch_size<span style="color:#f92672">=</span>extracted_patch_size,
                               max_patches<span style="color:#f92672">=</span>N, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    patches <span style="color:#f92672">=</span> extractor<span style="color:#f92672">.</span>transform(img[np<span style="color:#f92672">.</span>newaxis])
    <span style="color:#66d9ef">if</span> scale <span style="color:#f92672">!=</span> <span style="color:#ae81ff">1</span>:
        patches <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([transform<span style="color:#f92672">.</span>resize(patch, patch_size)
                            <span style="color:#66d9ef">for</span> patch <span style="color:#f92672">in</span> patches])
    <span style="color:#66d9ef">return</span> patches

negative_patches <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>vstack([extract_patches(im, <span style="color:#ae81ff">1000</span>, scale)
                              <span style="color:#66d9ef">for</span> im <span style="color:#f92672">in</span> images <span style="color:#66d9ef">for</span> scale <span style="color:#f92672">in</span> [<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>]])

</code></pre></div><h3 id="extract-hog-features">Extract HOG Features</h3>
<p>Scikit-Image&rsquo;s feature module offers a function skimage.feature.hog which extracts Histogram of Oriented Gradients (HOG) features for a given image. we combine the positive and negative set and compute the HOG features</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> feature   
<span style="color:#f92672">from</span> itertools <span style="color:#f92672">import</span> chain

X_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([feature<span style="color:#f92672">.</span>hog(im)
                    <span style="color:#66d9ef">for</span> im <span style="color:#f92672">in</span> chain(positive_patches,
                                    negative_patches)])
y_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])
y_train[:positive_patches<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

</code></pre></div><h3 id="training-a-svm-classifier">Training a SVM classifier</h3>
<p>We will use Scikit-Learn&rsquo;s LinearSVC with a grid search over a few choices of the C parameter:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.svm <span style="color:#f92672">import</span> LinearSVC
<span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> GridSearchCV

grid <span style="color:#f92672">=</span> GridSearchCV(LinearSVC(dual<span style="color:#f92672">=</span>False), {<span style="color:#e6db74">&#39;C&#39;</span>: [<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">4.0</span>, <span style="color:#ae81ff">8.0</span>]},cv<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
grid<span style="color:#f92672">.</span>fit(X_train, y_train)
grid<span style="color:#f92672">.</span>best_score_

</code></pre></div><p>We will take the best estimator and then build a model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>best_estimator_
model<span style="color:#f92672">.</span>fit(X_train, y_train)

</code></pre></div><h3 id="testing-on-a-new-image">Testing on a new image</h3>
<p>Now that we have built the Model we can test it on a new image to see how it detects the faces.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> io

img <span style="color:#f92672">=</span> io<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;testpic.jpg&#39;</span>,as_gray<span style="color:#f92672">=</span>True)
img <span style="color:#f92672">=</span> skimage<span style="color:#f92672">.</span>transform<span style="color:#f92672">.</span>rescale(img, <span style="color:#ae81ff">0.5</span>)
indices, patches <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>sliding_window(img))
patches_hog <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([feature<span style="color:#f92672">.</span>hog(patch) <span style="color:#66d9ef">for</span> patch <span style="color:#f92672">in</span> patches])
labels <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(patches_hog)

</code></pre></div><p>We are detecting the face by using a sliding window which goes over the image patches. Then we find the HOG feature of these patches. Finally, we run it through the classification model that we build and predict the face in the image. The image below is one of the test images. We can see that the classifier detected patches and most of them overlap the face in the image.</p>

    <img src="https://i.imgur.com/SkKwtRa.png"  class="center"  />


<p>To see the full code for this post check out this  <a href="https://github.com/Eklavya42/ComputerVision/tree/master/HOG">repository</a></p>

            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="tags/face-detection">face detection</a></span><span class="tag"><a href="tags/machine-learning">machine learning</a></span><span class="tag"><a href="tags/deep-learning">deep learning</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>1334 Words</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2019-03-11 17:15 &#43;0530</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h"></span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    

                    
                        <span class="button next">
                            <a href="/posts/2019/01/hopfield-network/">
                                <span class="button__text">Hopfield Network</span>
                                <span class="button__icon">→</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2020</span>
            
                <span><a href="">Eklavya Chopra</a></span>
            
            
                <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            
            <span> <a href="posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
        </div>
    </div>
    <div class="footer__inner">
        <div class="footer__content">
            <span>Powered by <a href="http://gohugo.io">Hugo</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="/bundle.min.36c441c050b35c23c624416764e32d939dd6ec19b227796b92c5fc150259bd58a24159eb9d41cb7ae5bb0d9db462842c62bfbac109a84b7ddb86dc6aaa52fc98.js" integrity="sha512-NsRBwFCzXCPGJEFnZOMtk53W7BmyJ3lrksX8FQJZvViiQVnrnUHLeuW7DZ20YoQsYr&#43;6wQmoS33bhtxqqlL8mA=="></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>



    </body>
</html>
