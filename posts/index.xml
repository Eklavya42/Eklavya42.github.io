<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Hi, I&#39;m Eklavya Chopra</title>
        <link>/posts/</link>
        <description>Recent content in Posts on Hi, I&#39;m Eklavya Chopra</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Mon, 11 Mar 2019 17:15:12 +0530</lastBuildDate>
        <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Face Detection using Histogram of Oriented Gradients</title>
            <link>/posts/2019/03/face-detection-using-histogram-of-oriented-gradients/</link>
            <pubDate>Mon, 11 Mar 2019 17:15:12 +0530</pubDate>
            
            <guid>/posts/2019/03/face-detection-using-histogram-of-oriented-gradients/</guid>
            <description>What is Histogram of Oriented Gradients (HOG)? Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005. Histogram of Oriented Gradients (HOG) is a feature descriptor used in image processing, mainly for object detection. A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information from it.
The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions.</description>
            <content type="html"><![CDATA[<h2 id="what-is-histogram-of-oriented-gradients-hog">What is Histogram of Oriented Gradients (HOG)?</h2>
<p>Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005. Histogram of Oriented Gradients (HOG) is a feature descriptor used in image processing, mainly for object detection. A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information from it.</p>
<p>The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions. The x and y derivatives of an image (Gradients) are useful because the magnitude of gradients is large around edges and corners due to abrupt change in intensity and we know that edges and corners pack in a lot more information about object shape than flat regions. So, the histograms of directions of gradients are used as features in this descriptor.</p>
<h2 id="workflow-of-face-detection-using-hog">Workflow of face detection using HOG</h2>
<p>Now that we know basic principle of Histogram of Oriented Gradients we will be moving into how we calculate the histograms and how these feature vectors, that are obtained from the HOG descriptor, are used by the classifier such a SVM to detect the concerned object.</p>

    <img src="https://i.imgur.com/5qac3GF.png"  class="center"  />


<center>  <figcaption>Steps for Object Detection with HOG</figcaption> </center>
<h2 id="how-histogram-of-oriented-gradientshog-works">How Histogram of Oriented Gradients(HOG) Works?</h2>
<h3 id="pre-processing">Pre-processing</h3>
<p>Preprocessing of image involves normalising the image but it is entirely optional. It is used to improve performance of the HOG descriptor. Since, here we are building a simple descriptor we don&rsquo;t use any normalisation in preprocessing.</p>
<h3 id="computing-gradient">Computing Gradient</h3>
<p>The first actual step in the HOG descriptor is to compute the image gradient in both the x and y direction.</p>
<p>Let us take an example. Say the pixel Q has values surrounding it as shown below:</p>

    <img src="https://i.imgur.com/sdA8be8.png"  class="center"  />


<p>We can calculate the Gradient magnitude for Q in x and y direction as follow:
$$
G_x = 100 -50 =50
$$</p>
<p>$$
G_y = 120 -70 =50
$$</p>
<p>We can get the magnitude of the gradient as:</p>
<p>$$
G= \sqrt{(G_x)^2 + (G_y)^2} = 70.7
$$
And the direction of the gradient as :</p>
<p>$$
\theta = arctan({\frac {G_y} {G_x}}) = 45^\circ
$$</p>
<h3 id="compute-histogram-of-gradients-in-88-cells">Compute Histogram of Gradients in 8×8 cells</h3>
<ul>
<li>
<p>The image is divided into 8×8 cell blocks and a histogram of gradients is calculated for each 8×8 cell block.</p>
</li>
<li>
<p>The histogram is essentially a vector of 9 buckets ( numbers ) corresponding to angles from \(0^ \circ\) to  \(180^ \circ\) (\(20^ \circ\) increments.)</p>
</li>
<li>
<p>The values of these 64 cells (8X8) are binned and cumulatively added into these 9 buckets.</p>
</li>
<li>
<p>This essentially reduces 64 values into 9 values.</p>
</li>
</ul>
<p>A great illustration of this is shown on  <a href="https://www.learnopencv.com/histogram-of-oriented-gradients/">learnopencv</a>. The following figure shows how it is done. The blue pixel encircled has an angle of \(80^ \circ\) and magnitude of 2. So it adds 2 to the 5th bin. The gradient at the pixel encircled using red has an angle of \(10^ \circ\) and magnitude of 4. Since \(0^ \circ\) is half way between \(0^ \circ\) and \(20^ \circ\), the vote by the pixel splits evenly into the two bins.</p>

    <img src="https://i.imgur.com/6zN14kq.png"  class="center"  />


<center>  <figcaption>Illustration of splitting of gradient magnitude according to gradient direction (Image source: https://www.learnopencv.com/histogram-of-oriented-gradients)</figcaption> </center>
<h3 id="block-normalisation">Block Normalisation</h3>
<p>After the creation of histogram of oriented gradients we need to something else too. Gradient is sensitive to overall lighting. If we say divide/multiply pixel values by some constant in order to make it lighter/ darker the gradient magnitude will change and so will histogram values. We want that histogram values be independent of lighting. Normalisation is done on the histogram vector v within a block. One of the following norms could be used:</p>
<ul>
<li>L1 norm</li>
<li>L2 norm</li>
<li>L2-Hys(Lowe-style clipped L2 norm)</li>
</ul>
<p>Now, we could simply normalise the 9×1 histogram vector but it is better to normalise a bigger sized block of 16×16. A 16×16 block has 4 histograms (8×8 cell results to one histogram) which can be concatenated to form a 36 x 1 element vector and normalised. The 16×16 window then moves by 8 pixels and a normalised 36×1 vector is calculated over this window and the process is repeated for the image.</p>
<h3 id="calculate-hog-descriptor-vector">Calculate HOG Descriptor vector</h3>
<ul>
<li>To calculate the final feature vector for the entire image patch, the 36×1 vectors are concatenated into one giant vector.</li>
<li>So, say if there was an input picture of size 64×64 then the 16×16 block has 7 positions horizontally and 7 position vertically.</li>
<li>In one 16×16 block we have 4 histograms which after normalisation concatenate to form a 36×1 vector.</li>
<li>This block moves 7 positions horizontally and vertically totalling it to 7×7 = 49 positions.</li>
<li>So when we concatenate them all into one giant vector we obtain a 36×49 = 1764 dimensional vector.</li>
</ul>
<p>This vector is now used to train classifiers such as SVM and then do object detection.</p>
<h3 id="visualization-of-hog-features">Visualization of HOG features</h3>
<p>Here is a snippet to visualise HOG features of an Image provided in  <a href="https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html">Scikit-Image&rsquo;s docs</a>  to visualize HOG features.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#f92672">from</span> skimage.feature <span style="color:#f92672">import</span> hog
<span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> data, exposure


image <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>astronaut()

fd, hog_image <span style="color:#f92672">=</span> hog(image, orientations<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, pixels_per_cell<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>),
                    cells_per_block<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), visualize<span style="color:#f92672">=</span>True, multichannel<span style="color:#f92672">=</span>True)

fig, (ax1, ax2) <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>), sharex<span style="color:#f92672">=</span>True, sharey<span style="color:#f92672">=</span>True)

ax1<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
ax1<span style="color:#f92672">.</span>imshow(image, cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>gray)
ax1<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Input image&#39;</span>)

<span style="color:#75715e"># Rescale histogram for better display</span>
hog_image_rescaled <span style="color:#f92672">=</span> exposure<span style="color:#f92672">.</span>rescale_intensity(hog_image, in_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>))

ax2<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
ax2<span style="color:#f92672">.</span>imshow(hog_image_rescaled, cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>gray)
ax2<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Histogram of Oriented Gradients&#39;</span>)
plt<span style="color:#f92672">.</span>show()


</code></pre></div><p>Output as follow :</p>

    <img src="https://i.imgur.com/EnNkv49.png"  class="center"  />


<center>  <figcaption>Visualisation of HOG features of image of the astronaut Eileen Collins.</figcaption> </center>
<h2 id="implementation">Implementation</h2>
<p>In this tutorial we will be performing a simple Face Detection using HOG features.<br>
We need to first train the classifier in order to do face detection so first we will need to have training set for the classifier.</p>
<h3 id="training-set">Training Set</h3>
<ul>
<li>Positive training samples</li>
</ul>
<p>Labelled Faces in the Wild dataset provided by Scikit-Learn consists of variety of faces which is perfect for our positive set.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> fetch_lfw_people
faces <span style="color:#f92672">=</span> fetch_lfw_people()
positive_patches <span style="color:#f92672">=</span> faces<span style="color:#f92672">.</span>images

</code></pre></div><ul>
<li>Negative training samples</li>
</ul>
<p>For Negative set we need images without face on them. Scikit-Image offers images which can be used in this case. To increase the size of negative set we extract patches of image at different scale using Patch Extractor from Scikit-Learn.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> data, transform
<span style="color:#f92672">from</span> sklearn.feature_extraction.image <span style="color:#f92672">import</span> PatchExtractor

imgs_to_use <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;camera&#39;</span>, <span style="color:#e6db74">&#39;text&#39;</span>, <span style="color:#e6db74">&#39;coins&#39;</span>, <span style="color:#e6db74">&#39;moon&#39;</span>,
               <span style="color:#e6db74">&#39;page&#39;</span>, <span style="color:#e6db74">&#39;clock&#39;</span>, <span style="color:#e6db74">&#39;immunohistochemistry&#39;</span>,
               <span style="color:#e6db74">&#39;chelsea&#39;</span>, <span style="color:#e6db74">&#39;coffee&#39;</span>, <span style="color:#e6db74">&#39;hubble_deep_field&#39;</span>]
images <span style="color:#f92672">=</span> [color<span style="color:#f92672">.</span>rgb2gray(getattr(data, name)())
          <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> imgs_to_use]


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_patches</span>(img, N, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, patch_size<span style="color:#f92672">=</span>positive_patches[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>shape):
    extracted_patch_size <span style="color:#f92672">=</span> tuple((scale <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>array(patch_size))<span style="color:#f92672">.</span>astype(int))
    extractor <span style="color:#f92672">=</span> PatchExtractor(patch_size<span style="color:#f92672">=</span>extracted_patch_size,
                               max_patches<span style="color:#f92672">=</span>N, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    patches <span style="color:#f92672">=</span> extractor<span style="color:#f92672">.</span>transform(img[np<span style="color:#f92672">.</span>newaxis])
    <span style="color:#66d9ef">if</span> scale <span style="color:#f92672">!=</span> <span style="color:#ae81ff">1</span>:
        patches <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([transform<span style="color:#f92672">.</span>resize(patch, patch_size)
                            <span style="color:#66d9ef">for</span> patch <span style="color:#f92672">in</span> patches])
    <span style="color:#66d9ef">return</span> patches

negative_patches <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>vstack([extract_patches(im, <span style="color:#ae81ff">1000</span>, scale)
                              <span style="color:#66d9ef">for</span> im <span style="color:#f92672">in</span> images <span style="color:#66d9ef">for</span> scale <span style="color:#f92672">in</span> [<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>]])

</code></pre></div><h3 id="extract-hog-features">Extract HOG Features</h3>
<p>Scikit-Image&rsquo;s feature module offers a function skimage.feature.hog which extracts Histogram of Oriented Gradients (HOG) features for a given image. we combine the positive and negative set and compute the HOG features</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> feature   
<span style="color:#f92672">from</span> itertools <span style="color:#f92672">import</span> chain

X_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([feature<span style="color:#f92672">.</span>hog(im)
                    <span style="color:#66d9ef">for</span> im <span style="color:#f92672">in</span> chain(positive_patches,
                                    negative_patches)])
y_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])
y_train[:positive_patches<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

</code></pre></div><h3 id="training-a-svm-classifier">Training a SVM classifier</h3>
<p>We will use Scikit-Learn&rsquo;s LinearSVC with a grid search over a few choices of the C parameter:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.svm <span style="color:#f92672">import</span> LinearSVC
<span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> GridSearchCV

grid <span style="color:#f92672">=</span> GridSearchCV(LinearSVC(dual<span style="color:#f92672">=</span>False), {<span style="color:#e6db74">&#39;C&#39;</span>: [<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">4.0</span>, <span style="color:#ae81ff">8.0</span>]},cv<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
grid<span style="color:#f92672">.</span>fit(X_train, y_train)
grid<span style="color:#f92672">.</span>best_score_

</code></pre></div><p>We will take the best estimator and then build a model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>best_estimator_
model<span style="color:#f92672">.</span>fit(X_train, y_train)

</code></pre></div><h3 id="testing-on-a-new-image">Testing on a new image</h3>
<p>Now that we have built the Model we can test it on a new image to see how it detects the faces.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> io

img <span style="color:#f92672">=</span> io<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;testpic.jpg&#39;</span>,as_gray<span style="color:#f92672">=</span>True)
img <span style="color:#f92672">=</span> skimage<span style="color:#f92672">.</span>transform<span style="color:#f92672">.</span>rescale(img, <span style="color:#ae81ff">0.5</span>)
indices, patches <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>sliding_window(img))
patches_hog <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([feature<span style="color:#f92672">.</span>hog(patch) <span style="color:#66d9ef">for</span> patch <span style="color:#f92672">in</span> patches])
labels <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(patches_hog)

</code></pre></div><p>We are detecting the face by using a sliding window which goes over the image patches. Then we find the HOG feature of these patches. Finally, we run it through the classification model that we build and predict the face in the image. The image below is one of the test images. We can see that the classifier detected patches and most of them overlap the face in the image.</p>

    <img src="https://i.imgur.com/SkKwtRa.png"  class="center"  />


<p>To see the full code for this post check out this  <a href="https://github.com/Eklavya42/ComputerVision/tree/master/HOG">repository</a></p>
]]></content>
        </item>
        
        <item>
            <title>Hopfield Network</title>
            <link>/posts/2019/01/hopfield-network/</link>
            <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
            
            <guid>/posts/2019/01/hopfield-network/</guid>
            <description>Hopfield Network is a recurrent neural network with bipolar threshold neurons. In this article, we will go through in depth along with an implementation. Before going into Hopfield network, we will revise basic ideas like Neural network and perceptron.
A neural network is a mathematical model or computational model inspired by biological neural networks. It consists of an interconnected group of artiﬁcial neurons. The sructure and functioning of the central nervous system constituing neurons, axons, dentrites and syanpses which make up the processing parts of the biological neural networks were the original inspiration that led to the developement of computational models of neural networks.</description>
            <content type="html"><![CDATA[<p><strong>Hopfield Network</strong>  is a recurrent neural network with bipolar threshold neurons. In this article, we will go through in depth along with an implementation. Before going into Hopfield network, we will revise basic ideas like Neural network and perceptron.</p>
<p>A  <strong>neural network</strong>  is a mathematical model or computational model inspired by biological neural networks. It consists of an interconnected group of artiﬁcial neurons. The sructure and functioning of the central nervous system constituing neurons, axons, dentrites and syanpses which make up the processing parts of the biological neural networks were the original inspiration that led to the developement of computational models of neural networks.</p>
<p>The ﬁrst computational model of a  <strong>neuron</strong>  was presented in 1943 by  <strong>W. Mc Culloch</strong>  and  <strong>W. Pitts</strong>. They called this model threshold logic.The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.</p>

    <img src="https://i.imgur.com/Qi53ISi.png"  alt="Neuron"  class="center"  />


<p>In 1958,  <strong>Rossenblatt</strong>  conceived the  <strong>Perceptron</strong>, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtractio. His work had big repercussion but in 1969 a violent critic by Minsky and Papert was published.</p>
<p>The work on neural network was slow down but  <strong>John Hopﬁeld</strong>  convinced of the power of neural network came out with his model in 1982 and boost research in this ﬁeld. Hopﬁeld Network is a particular case of Neural Network. It is based on physics, inspired by spin system.</p>

    <img src="https://i.imgur.com/MybBfsm.png"  alt="Neural network"  class="center"  />


<h2 id="the-network">The Network</h2>
<p><strong>Hopfield Network</strong>  is a  <strong>recurrent neural network</strong>  with  <strong>bipolar threshold neurons</strong>. Hopﬁeld network consists of a set of interconnected neurons which update their activation values asynchronously. The activation values are binary, usually {-1,1}. The update of a unit depends on the other units of the network and on itself.</p>
<p>A neuron in Hopfield model is  <strong>binary</strong>  and defined by the  <strong>standard McCulloch-Pitts model</strong>  of a neuron:</p>
<p>\[n_i (t+1)= \theta(\sum_{j}w_{ij} n_j (t) - \mu_i)  \tag{1}\]</p>
<p>where n<sub>i</sub>(t+1) is the i<sup>th</sup> neuron at time t+1, n<sub>j</sub>(t) is the j<sup>th</sup> neuron at time t, w<sub>ij</sub> is the weight matrix called <em>synaptic weights</em> , θ is the step function and μ is the bias. In the Hopfield model the neurons have a binary output taking values -1 and 1. Thus the model has following form:</p>
<p>\[S_i(t+1) = sgn(\sum_{j}w_{ij} S_j(t) - \vartheta_i) \tag{2} \]</p>
<p>where S<sub>i</sub> and n<sub>i</sub> are related through the formula: S<sub>i</sub> = 2n<sub>i</sub> - 1 (Since n<sub>i</sub> ϵ [0,1] and S<sub>i</sub> ϵ [-1,1] ).  ϑ<sub>i</sub> is the threshold, so if the input is above the threshold it will fire 1. So here S represents the neurons which were represented as n in equation 1. The <em>sgn</em> sign here is the signum function which is described as follow:
$$
sgn(x) =
\begin{cases}
-1 &amp; \text{if $x$ &lt; 0,}\\\<br>
0 &amp; \text{if $x$ = 0,}\\\<br>
1 &amp; \text{if $x$ &gt; 0}
\end{cases}
$$</p>
<p>For ease of analysis in this post we will drop the threshold (ϑ<sub>i </sub>= 0) as we will analyse mainly random patterns and thresholds are not useful in this context. In this case the model is written as:</p>
<p>$$
S_i(t+1) = sgn(\sum_{j}w_{ij} S_j(t) ) \tag{3}
$$</p>
<h2 id="training-the-network">Training the Network</h2>
<p>In this post we are looking at Auto-associative model of Hopfield Network. It can store useful information in memory and later it is able to reproduce this information from partially broken patterns.</p>
<p>For training procedure, it doesn’t require any iterations. It includes just an outer product between input vector and transposed input vector to fill the weighted matrix w<sub>ij</sub>  (or synaptic weights) and in case of many patterns it is as follow:</p>
<p>$$
w_{i,j} = \frac{1}{N} \sum_{\mu=1}^p \epsilon_i^\mu\epsilon_j^\mu \tag{4}
$$</p>
<p>where, ε is the pattern and <em>p</em> = total number of patterns.</p>
<p>The main advantage of Auto-associative network is that it is able to recover pattern from the memory using just a partial information about the pattern. There are two main approaches to this situation.<br>
We update the neurons as specified in equation 3:</p>
<ul>
<li><strong>Syncronously</strong>: Update all the neurons simultaneously at each time step;</li>
<li><strong>Asyncronously</strong>: At each time step, select at random, a unit i and update it.</li>
</ul>
<h2 id="implementation">Implementation</h2>
<p>Now that we have covered the basics lets start implementing Hopfield network.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

nb_patterns <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>   <span style="color:#75715e"># Number of patterns to learn</span>
pattern_width <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
pattern_height <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
max_iterations <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>

<span style="color:#75715e"># Define Patterns</span>
patterns <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([
   [<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1.</span>],   <span style="color:#75715e"># Letter D</span>
   [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1.</span>],    <span style="color:#75715e"># Letter J</span>
   [<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>],     <span style="color:#75715e"># Letter C</span>
   [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>],], <span style="color:#75715e"># Letter M</span>
   dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float)

</code></pre></div><p>So we import the necessary libraries and define the patterns we want the network to learn. Here, we are defining 4 patterns. We can visualise them with the help of the code below:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Show the patterns</span>
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, nb_patterns, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">10</span>))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(nb_patterns):
    ax[i]<span style="color:#f92672">.</span>matshow(patterns[i]<span style="color:#f92672">.</span>reshape((pattern_height, pattern_width)), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
    ax[i]<span style="color:#f92672">.</span>set_xticks([])
    ax[i]<span style="color:#f92672">.</span>set_yticks([])

</code></pre></div><p>Which gives the out put as :</p>

    <img src="https://i.imgur.com/u6T2YkH.png"  class="center"  />


<p>We now train the network by filling the weight matrix as defined in the equation 4</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Train the network</span>
W <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((pattern_width <span style="color:#f92672">*</span> pattern_height, pattern_width <span style="color:#f92672">*</span> pattern_height))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(pattern_width <span style="color:#f92672">*</span> pattern_height):
    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(pattern_width <span style="color:#f92672">*</span> pattern_height):
        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> j <span style="color:#f92672">or</span> W[i, j] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0.0</span>:
            <span style="color:#66d9ef">continue</span>

        w <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>

        <span style="color:#66d9ef">for</span> n <span style="color:#f92672">in</span> range(nb_patterns):
            w <span style="color:#f92672">+=</span> patterns[n, i] <span style="color:#f92672">*</span> patterns[n, j]

        W[i, j] <span style="color:#f92672">=</span> w <span style="color:#f92672">/</span> patterns<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]  
        W[j, i] <span style="color:#f92672">=</span> W[i, j]  

</code></pre></div><p>Now that we have trained the network. We will create a corrupted pattern to test on this network.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Test the Network</span>
<span style="color:#75715e"># Create a corrupted pattern S</span>
S <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(    [<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>],     
   dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float)

<span style="color:#75715e"># Show the corrupted pattern</span>
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
ax<span style="color:#f92672">.</span>matshow(S<span style="color:#f92672">.</span>reshape((pattern_height, pattern_width)), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)

</code></pre></div><p>The corrupted pattern we take here simply edditing some bits in the pattern array of letter C. We can see the corrupted pattern as follow:</p>

    <img src="https://i.imgur.com/XROw3hO.png"  class="center"  />


<p>We pass the corrupted pattern through the network and it is updated as defined in the equation 3. Thus, after each iteration some update is applied to the corrupted matrix. We take the hamming distance of the corrupted pattern which is being updated every time and all the patterns. And then we decide the closest pattern in terms of least hamming distance.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">h <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((pattern_width <span style="color:#f92672">*</span> pattern_height))
<span style="color:#75715e">#Defining Hamming Distance matrix for seeing convergence</span>
hamming_distance <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((max_iterations,nb_patterns))
<span style="color:#66d9ef">for</span> iteration <span style="color:#f92672">in</span> range(max_iterations):
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(pattern_width <span style="color:#f92672">*</span> pattern_height):
        i <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(pattern_width <span style="color:#f92672">*</span> pattern_height)
        h[i] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(pattern_width <span style="color:#f92672">*</span> pattern_height):
            h[i] <span style="color:#f92672">+=</span> W[i, j]<span style="color:#f92672">*</span>S[j]
        S <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(h<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(nb_patterns):
        hamming_distance[iteration, i] <span style="color:#f92672">=</span> ((patterns <span style="color:#f92672">-</span> S)[i]<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>sum()   

    fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
    ax<span style="color:#f92672">.</span>matshow(S<span style="color:#f92672">.</span>reshape((pattern_height, pattern_width)), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
hamming_distance

</code></pre></div>
    <img src="https://i.imgur.com/dMnz7lH.png"  class="center"  />


<p>Here we see that the hamming distance between corrupted pattern and third pattern i.e. letter C has become 0 after few iterations thus correcting the corrupted pattern.</p>
<p>We can also see the plot for all the hamming distances below:</p>

    <img src="https://i.imgur.com/iI89O7y.png"  class="center"  />


]]></content>
        </item>
        
        <item>
            <title>Object Detection Techniques</title>
            <link>/posts/2018/04/object-detection-techniques/</link>
            <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
            
            <guid>/posts/2018/04/object-detection-techniques/</guid>
            <description>In this article, we will understand what is object detection, why we need to do object detection and the basic idea behind various techniques used to solved this problem. We start with the basic techniques like Viola Jones face detector to some of the advanced techniques like Single Shot Detector. Some the the techniques are:
 Viola Jones face detector Object Detection using Histogram of Oriented Gradients (HOG) Features Scale-invariant feature transform (SIFT) AlexNet Region-based Convolutional Network (R-CNN) You Only Look Once (YOLO) Single Shot Detector (SSD)  What is Object Detection?</description>
            <content type="html"><![CDATA[<p>In this article, we will understand what is object detection, why we need to do object detection and the basic idea behind various techniques used to solved this problem. We start with the basic techniques like Viola Jones face detector to some of the advanced techniques like Single Shot Detector. Some the the techniques are:</p>
<ul>
<li>Viola Jones face detector</li>
<li>Object Detection using Histogram of Oriented Gradients (HOG) Features</li>
<li>Scale-invariant feature transform (SIFT)</li>
<li>AlexNet</li>
<li>Region-based Convolutional Network (R-CNN)</li>
<li>You Only Look Once (YOLO)</li>
<li>Single Shot Detector (SSD)</li>
</ul>
<h2 id="what-is-object-detection">What is Object Detection?</h2>
<p>The formal definition for object detection is as follows:</p>
<blockquote>
<p>A Computer Vision technique to locate the presence of objects on images or videos. Object Detection comprises of two things i.e. Image Classification and Object Localization.</p>
</blockquote>
<p><strong>Image Classification</strong> answers the question &quot; What is in the picture/frame?&rdquo;. It takes an image and predicts the object in an image. For example, in the pictures below we can build a classifier that can detect a person in the picture and a bicycle.</p>

    <img src="https://i.imgur.com/rdfswAP.png"  alt="Object Detection in Working"  class="center"  style="border-radius: 8px;"  />


<p>But if both of them are in the same image then it becomes a problem. We could train a multilabel classifier but we still don’t know the positions of bicycle or person. The task of locating the object in the image is called <strong>Object localisation</strong>.</p>
<h2 id="why-we-need-object-detection">Why we need object detection?</h2>
<p>Object detection is a widely used technique in production systems. There are variants of object detection problem such as:</p>
<ul>
<li>Image classification</li>
<li>Image segmentation</li>
<li>Object detection has its own place and it is used as follows:</li>
</ul>
<p>An image has multiple objects but every application has a focus on a particular thing such as a face detection application is focused on finding a face, a traffic control system is focused on vechiles, an driving technology is focused on differentiating between vehicles and living beings. In the same line, Object detection technique helps to identify the image segment that the application needs to focus on.</p>
<p>It can be used to reduce the dimension of the image to only capture the object of interest and hence, improving the execution time greatly.</p>
<h2 id="object-detection-techniques">Object Detection Techniques</h2>
<p>Generally, Object detection is achieved by using either machine-learning based approaches or Deep learning based approaches.</p>
<h3 id="machine-learning-based-techniques">Machine Learning Based techniques</h3>
<p>In this approach, we define the features and then train the classifier (such as SVM) on the feature-set. Following are the machine learning based object detection techniques:</p>
<h4 id="1-viola-jones-face-detector-2001">1. Viola Jones face detector (2001)</h4>
<ul>
<li>It was the first efficient face detection algorithm to provide competitive results.</li>
<li>They hardcoded the features of the face (Haar Cascades) and then trained an SVM classifier on the featureset. Then they used that classifier to detect faces.</li>
<li>The downside of this algorithm was that is was unable to detect faces in other orientation or arrangement (such as wearing a mask, face tilted, etc.)</li>
</ul>
<h4 id="2-object-detection-using-histogram-of-oriented-gradients-hog-features">2. Object Detection using Histogram of Oriented Gradients (HOG) Features</h4>
<ul>
<li>Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005.</li>
<li>The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions.</li>
<li>The image is divided into small connected regions called cells, and for the pixels within each cell, a histogram of gradient directions is compiled. A descriptor is assigned to each detector window. This descriptor consists of all the cell histograms for each block in the detector window. The detector window descriptor is used as information for object recognition. Training and testing of classifiers such as SVM happens using this descriptor.</li>
</ul>

    <img src="https://i.imgur.com/MtIoUfK.png"  alt="HOG features"  class="center"  style="border-radius: 8px;"  />


<ul>
<li>Despite being good in many applications, it still used hand coded features which failed in a more generalized setting with much noise and distractions in the background.</li>
</ul>
<h4 id="3-scale-invariant-feature-transform-sift">3. Scale-invariant feature transform (SIFT)</h4>
<p>SIFT was created by David Lowe from the University British Columbia in 1999.The SIFT approach, for image feature generation, takes an image and transforms it into a large collection of local feature vectors. Each of these feature vectors is invariant to any scaling, rotation or translation of the image. There are four steps involved in the SIFT algorithm:</p>
<ul>
<li>Scale-space peak selection: Potential location for finding features.</li>
<li>Keypoint Localization: Accurately locating the feature keypoints.</li>
<li>Orientation Assignment: Assigning orientation to keypoints.</li>
<li>Keypoint descriptor: Describing the keypoints as a high dimensional vector.</li>
</ul>
<p>These resulting vectors are known as SIFT keys and are used in a nearest-neighbour approach to identify possible objects in an image.</p>

    <img src="https://i.imgur.com/DxRth6Z.png"  alt="SIFT features"  class="center"  style="border-radius: 8px;"  />


<h3 id="deep-learning-based-techniques">Deep Learning Based techniques</h3>
<p>Deep Learning techniques are able to do end-to-end object detection without specifically defining features, and are typically based on convolutional neural networks (CNN). A Convolutional Neural Network (CNN, or ConvNet) is a special kind of multi-layer neural networks, designed to recognize visual patterns directly from pixel images.</p>
<h4 id="1-alexnet">1. AlexNet</h4>
<p>In 2012, AlexNet significantly outperformed all prior competitors at ImageNet Large Scale Visual Recognition Challenge(ILSVRC) and won the challenge. Convolutional Neural Networks became the gold standard for image classification after Kriszhevsky&rsquo;s CNN&rsquo;s performance during ImageNet.</p>

    <img src="https://i.imgur.com/qIsJDE6.png"  alt="Alexnet"  class="center"  style="border-radius: 8px;"  />


<h4 id="2-region-based-convolutional-network-r-cnn">2. Region-based Convolutional Network (R-CNN)</h4>
<p>CNNs were too slow and computationally very expensive. R-CNN solves this problem by using an object proposal algorithm called Selective Search which reduces the number of bounding boxes that are fed to the classifier to close to 2000 region proposals.</p>
<p>In R-CNN, the selective search method developed by J.R.R. Uijlings and al. (2012) is an alternative to exhaustive search in an image to capture object location. It looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.</p>
<p>The main idea is composed of two steps. First, using selective search, it identifies a manageable number of bounding-box object region candidates (region of interest). And then it extracts CNN features from each region independently for classification.</p>

    <img src="https://i.imgur.com/aQ5a9MO.png"  alt="R-CNN"  class="center"  style="border-radius: 8px;"  />


<p>R-CNN was improved over the time for better performance. Fast Region-based Convolutional Network (Fast R-CNN) developed by R. Girshick (2015) reduced the time consumption related to the high number of models necessary to analyse all region proposals in R-CNN.</p>
<h4 id="3-you-only-look-once-yolo">3. You Only Look Once (YOLO)</h4>
<p>The YOLO model (J. Redmon et al., 2016) directly predicts bounding boxes and class probabilities with a single network in a single evaluation. They reframe the object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities.</p>
<p>YOLO divides each image into a grid of S x S and each grid predicts N bounding boxes and confidence. The confidence score tells us how certain it is that the predicted bounding box actually encloses some object.</p>

    <img src="https://i.imgur.com/09WoKLp.png"  alt="YOLO"  class="center"  style="border-radius: 8px;"  />


<p>Over time, it has become faster and better, with its versions named as: YOLO V1, YOLO V2 and YOLO V3. YOLO V2 is better than V1 in terms of accuracy and speed. YOLO V3 is more accurate than V2.</p>
<h4 id="4-single-shot-detectorssd">4. Single Shot Detector(SSD)</h4>
<p>SSD model was published (by Wei Liu et al.) in 2015, shortly after the YOLO model, and was also later refined in a subsequent paper.</p>
<p>Unlike YOLO, SSD does not split the image into grids of arbitrary size but predicts offset of predefined anchor boxes for every location of the feature map. Each box has a fixed size and position relative to its corresponding cell. All the anchor boxes tile the whole feature map in a convolutional manner.</p>
<p>Feature maps at different levels have different receptive field sizes. The anchor boxes on different levels are rescaled so that one feature map is only responsible for objects at one particular scale.</p>

    <img src="https://i.imgur.com/aCDTAje.png"  alt="YOLO"  class="center"  style="border-radius: 8px;"  />


]]></content>
        </item>
        
    </channel>
</rss>
