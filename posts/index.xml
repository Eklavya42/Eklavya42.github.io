<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Hi, I&#39;m Eklavya Chopra</title>
        <link>/posts/</link>
        <description>Recent content in Posts on Hi, I&#39;m Eklavya Chopra</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Thu, 04 Jun 2020 17:15:12 +0530</lastBuildDate>
        <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Linear Regression</title>
            <link>/posts/2020/06/linear-regression/</link>
            <pubDate>Thu, 04 Jun 2020 17:15:12 +0530</pubDate>
            
            <guid>/posts/2020/06/linear-regression/</guid>
            <description>This post will cover the linear regression implementation (From Scratch using Pytorch).
Linear Regression Equation : Overview Linear regression attempts to fit a line of best fit to a data set, using one or more features as coefficients for a linear equation. It is an approach for modelling the relationship between dependent variable and independent variables.
In a linear regression model, each target (dependent) variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :</description>
            <content type="html"><![CDATA[<p>This post will cover the linear regression implementation (From Scratch using Pytorch).</p>
<h2 id="linear-regression-equation--overview">Linear Regression Equation : Overview</h2>
<p>Linear regression attempts to fit a line of best fit to a data set, using one or more features as coefficients for a linear equation. It is an approach for modelling the relationship between <em>dependent</em> variable and <em>independent</em> variables.</p>
<p>In a linear regression model, each target (dependent) variable   is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :</p>
<p>$$
Y = X.W^T + b \tag{1}
$$</p>
<p>Where,</p>
<p>$$
Y = \begin{bmatrix}y_1\\y_2\\.\\.\\y_n\end{bmatrix}_{n\times1}   X  = \begin{bmatrix}x_{11} &amp; x_{12} &amp; . &amp; . &amp; x_{1n} \\x_{21} &amp; x_{22} &amp; . &amp; . &amp; x_{2n} \\. &amp; . &amp; . &amp; . &amp; .\\.&amp; .&amp; . &amp; . &amp; .\\x_{n1} &amp; x_{n2} &amp; . &amp; . &amp; x_{nn}\end{bmatrix}_{n\times n} W^T = \begin{bmatrix}w_1\\ w_2\\.\\.\\w_n\end{bmatrix}_{n\times 1}  b  = \begin{bmatrix}b_1\\ b_2\\.\\.\\b_n\end{bmatrix}_{n\times 1}<br>
$$</p>
<p>We get the following expansion of the equation 1 :
$$
y_1 = w_1x_{11} + w_2x_{12} + . ..+ w_nx_{1n} + b_1
$$</p>
<p>Now, lets take an example for better explanation. Here, we take a look at advertisement data which has the following data :</p>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=5&hideInput=true" title="Jovian Viewer" height="216" width="800" frameborder="0" scrolling="auto"></iframe>
<p>The above data consists of sales of a particular product along with advertisement budget for the product in TV, radio and newspaper media. Our objective is to increase the sales of the product and we can control the budget of advertisement. So, if we determine the relationship between advertisement budget and sales, we can figure out how to increase the sales of a product by introducing changes in the advertisement budget. Here, the <em>independent variables</em> \((x_i)\) will be the advertisement budget for each of the three media and the <em>dependent variable</em>\((y)\) will be the sales of the product. The relationship between independent and dependent variables in this data can be defined as :
$$
y = w_1x_1 + w_2x_2 + w_3x_3 + b \tag{2}
$$
where, \( y\) is sales and \(x_1,x_2,x_3\)  are advertisement budgets for TV, radio and newspaper respectively. The above equation can be written in matrix form as :
$$
y = X.W^T +b
$$</p>
<p>where
$$
X = \begin{bmatrix}x_1 &amp; x_2 &amp; x_3\end{bmatrix}_{1\times 3} \hspace{1cm}  W = \begin{bmatrix}w_1 &amp; w_2 &amp; w_3\end{bmatrix}_{1\times 3}
$$</p>
<p>Now that we have discussed the linear regression equation, lets move on towards implementation and discuss the concepts implemented.</p>
<h2 id="implementation-from-scratch-using-pytorch">Implementation from Scratch (using Pytorch)</h2>
<h3 id="importing-relevant-libraries">Importing relevant libraries</h3>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=1" title="Jovian Viewer" height="248" width="800" frameborder="0" scrolling="auto"></iframe>
<h3 id="dataset">Dataset</h3>
<p>I am using Advertisement Data which was also used in <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/data.html">ISLR book</a>.</p>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=3" title="Jovian Viewer" height="183" width="800" frameborder="0" scrolling="auto"></iframe>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch/v/2&cellId=4" title="Jovian Viewer" height="350" width="800" frameborder="0" scrolling="auto"></iframe>
<p>We can remove the inbuilt index in the data.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># removing the inbuilt index column</span>
df<span style="color:#f92672">.</span>drop(<span style="color:#e6db74">&#39;Unnamed: 0&#39;</span>, axis <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, inplace<span style="color:#f92672">=</span>True)
df<span style="color:#f92672">.</span>head()
</code></pre></div><p>Now lets get some more  information on the dataset.</p>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=6" title="Jovian Viewer" height="370" width="800" frameborder="0" scrolling="auto"></iframe>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=7" title="Jovian Viewer" height="348" width="800" frameborder="0" scrolling="auto"></iframe>
<p>Lets divide the dataset into target and input variables and then split it into test and train data</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">x <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>drop(<span style="color:#e6db74">&#39;sales&#39;</span>, axis <span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>values
y <span style="color:#f92672">=</span> df[[<span style="color:#e6db74">&#39;sales&#39;</span>]]<span style="color:#f92672">.</span>values

<span style="color:#75715e"># Converting the numpy array features to pytorch tensors.</span>
inputs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>from_numpy(x)<span style="color:#f92672">.</span>float()
targets <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>from_numpy(y)<span style="color:#f92672">.</span>float()

<span style="color:#75715e"># Split Data into train and test</span>
X_train,X_test,y_train,y_test <span style="color:#f92672">=</span> train_test_split(inputs,targets,test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.20</span>,random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</code></pre></div><p>For the data split, I decided on 80–20 split for train and test set.</p>
<h3 id="weights-and-bias">Weights and Bias</h3>
<p>Our model is simply a function that performs a matrix multiplication of the  <code>inputs</code>  and the weights  <code>w</code>  (transposed) and adds the bias  <code>b</code>  (see equation 2). So we initialize the
Weight matrix and bias</p>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=13" title="Jovian Viewer" height="225" width="800" frameborder="0" scrolling="auto"></iframe>
<h3 id="model">Model</h3>
<p>We can define the model as follows:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">model</span>(x):
    <span style="color:#66d9ef">return</span> x <span style="color:#960050;background-color:#1e0010">@</span> w<span style="color:#f92672">.</span>t() <span style="color:#f92672">+</span> b
</code></pre></div><p><code>@</code>  represents matrix multiplication in PyTorch, and the  <code>.t</code>  method returns the transpose of a tensor. The matrix obtained by passing the input data into the model is a set of predictions for the target variables.(see equation 2)</p>
<h3 id="loss-function">Loss Function</h3>
<p>We need a way to evaluate how well our model is performing. We can compare the model&rsquo;s predictions with the actual targets, using the following method:</p>
<ul>
<li>Calculate the difference between the two matrices (preds and targets).</li>
<li>Square all elements of the difference matrix to remove negative values.</li>
<li>Calculate the average of the elements in the resulting matrix.</li>
</ul>
<p>The result is a single number, known as the  <strong>mean squared error</strong>  (MSE).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mse</span>(t1, t2):
    diff <span style="color:#f92672">=</span> t1<span style="color:#f92672">-</span>t2
    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>sum(diff<span style="color:#f92672">*</span>diff)<span style="color:#f92672">/</span>diff<span style="color:#f92672">.</span>numel()
</code></pre></div><p><code>torch.sum</code> returns the sum of all the elements in a tensor, and the <code>.numel</code> method returns the number of elements in a tensor.</p>
<p>Let&rsquo;s compute the mean squared error for the current predictions of our model.</p>
 <iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=20" title="Jovian Viewer" height="241" width="800" frameborder="0" scrolling="auto"></iframe>
<p>Here’s how we can interpret the result: On average, each element in the prediction differs from the actual target by about 276.829088 (square root of the loss 76634.3438). And that’s pretty bad, considering the numbers we are trying to predict are themselves in the range 1-27. Also, the result is called the loss, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model.</p>
<h3 id="gradient-descent">Gradient Descent</h3>
<p>We’ll now minimize the <em>loss function</em> using the <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> algorithm. Intuitively, gradient descent takes small, linear steps down the slope of a function in each feature dimension, with the size of each step determined by the partial derivative of the cost function with respect to that feature and a learning rate multiplier \(\eta\). If tuned properly, the algorithm converges on a global minimum by iteratively adjusting feature weights \(\theta\) of the cost function, as shown here for two feature dimensions:
$$
\theta_0  := \theta_0 - \eta\frac{\partial}{\partial\theta_0} J(\theta_0,\theta_1)
$$
$$
\theta_1 := \theta_1 - \eta\frac{\partial}{\partial\theta_1} J(\theta_0,\theta_1)
$$</p>
<p>Given that :
$$
h_\theta(x)  = \theta_0 + \theta_1x_1
$$
$$
J(\theta)  = \frac{1}{2m}\sum\limits_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2
$$</p>
<p>For more about it read here : <a href="https://github.com/cleor41/CS229_Notes/blob/master/lectures/cs229-notes1.pdf" target="_blank">CS229_Notes</a></p>
<p>With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases, because they have  <code>requires_grad</code>  set to  <code>True</code>.</p>
<h3 id="adjust-weights-and-biases-using-gradient-descent">Adjust weights and biases using gradient descent</h3>
<p>We&rsquo;ll reduce the loss and improve our model using the gradient descent optimization algorithm, which has the following steps:</p>
<ol>
<li>
<p>Generate predictions</p>
</li>
<li>
<p>Calculate the loss</p>
</li>
<li>
<p>Compute gradients w.r.t the weights and biases</p>
</li>
<li>
<p>Adjust the weights by subtracting a small quantity proportional to the gradient</p>
</li>
<li>
<p>Reset the gradients to zero</p>
</li>
</ol>
<p>Let&rsquo;s implement the above step by step.</p>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=30" title="Jovian Viewer" height="529" width="800" frameborder="0" scrolling="auto"></iframe>
<p>Let&rsquo;s take a look at the loss after 1 epoch.</p>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=35" title="Jovian Viewer" height="181" width="800" frameborder="0" scrolling="auto"></iframe>
<p>We have already achieved a significant reduction in the loss, simply by adjusting the weights and biases slightly using gradient descent.</p>
<h3 id="train-for-multiple-epochs">Train for multiple epochs</h3>
<p>To reduce the loss further, we can repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an epoch. Let&rsquo;s train the model for 1000 epochs.</p>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=38" title="Jovian Viewer" height="657" width="800" frameborder="0" scrolling="auto"></iframe>
<p>Now lets see the final loss :</p>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=39" title="Jovian Viewer" height="209" width="800" frameborder="0" scrolling="auto"></iframe>
<p>As you can see, the loss is now much lower than what we started out with.</p>
<h3 id="testing">Testing</h3>
<p>Let&rsquo;s look at the model&rsquo;s predictions and compare them with the targets.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">y_preds <span style="color:#f92672">=</span> model(X_test)
</code></pre></div><iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=46" title="Jovian Viewer" height="141" width="800" frameborder="0" scrolling="auto"></iframe>
<p>Lets plot actual vs predicted :</p>
<iframe src="https://jovian.ml/embed?url=https://jovian.ml/eklavya42/lr-advertisement-pytorch-scratch/v/2&cellId=50" title="Jovian Viewer" height="517" width="800" frameborder="0" scrolling="auto"></iframe>
<p>Now this whole process can be done using pytorch builtins.  Check it out :</p>
<center>
<a href="https://jovian.ml/eklavya42/lr-advertisement-pytorch" target="_blank">Linear Regression Using Pytorch Builtins</a>
</center>
]]></content>
        </item>
        
        <item>
            <title>Object Detection using SIFT</title>
            <link>/posts/2019/03/object-detection-using-sift/</link>
            <pubDate>Sat, 16 Mar 2019 17:15:12 +0530</pubDate>
            
            <guid>/posts/2019/03/object-detection-using-sift/</guid>
            <description>Object Detection using SIFT algorithm SIFT (Scale Invariant Feature Transform) is a feature detection algorithm in computer vision to detect and describe local features in images. It was created by David Lowe from the University British Columbia in 1999. David Lowe presents the SIFT algorithm in his original paper titled Distinctive Image Features from Scale-Invariant Keypoints.
Image features extracted by SIFT are reasonably invariant to various changes such as their llumination image noise, rotation, scaling, and small changes in viewpoint.</description>
            <content type="html"><![CDATA[<h1 id="object-detection-using-sift-algorithm">Object Detection using SIFT algorithm</h1>
<p>SIFT (Scale Invariant Feature Transform) is a  <strong>feature detection algorithm</strong>  in computer vision to detect and describe local features in images. It was created by David Lowe from the University British Columbia in  <strong>1999</strong>. David Lowe presents the SIFT algorithm in his original paper titled  <a href="https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">Distinctive Image Features from Scale-Invariant Keypoints</a>.</p>
<p>Image features extracted by SIFT are reasonably invariant to various changes such as their  <strong>llumination image noise</strong>,  <strong>rotation</strong>,  <strong>scaling</strong>, and  <strong>small changes in viewpoint</strong>.</p>
<p>There are four main stages involved in SIFT algorithm :</p>
<ol>
<li>Scale-space extrema detection</li>
<li>Keypoint localization</li>
<li>Orientation Assignment</li>
<li>Keypoint descriptor</li>
</ol>
<p>We will now examine these stages in detail.</p>
<h2 id="sift-algorithm--explained">SIFT Algorithm : Explained</h2>
<h2 id="1-scale-space-extrema-detection">1. Scale-space extrema detection</h2>
<p>Before going into this, we will visit the idea of scale space theory and then, see how it has been used in SIFT.</p>
<h3 id="scale-space">Scale-space</h3>
<p>Scale-space theory is a framework for multiscale image representation, which has been developed by the computer vision community with complementary motivations from physics and biologic vision. The idea is to handle the multiscale nature of real-world objects, which implies that objects may be perceived in different ways depending on the scale of observation.</p>
<h3 id="scale-space-in-sift">Scale-space in SIFT</h3>
<p>The  <strong>first stage is to identify locations and scales that can be repeatably assigned under differing views of the same object</strong>. Detecting locations that are invariant to scale change of the image can be accomplished by searching for stable features across all possible scales, using a continuous function of scale known as scale space.</p>
<p>The scale space is defined by the function:</p>
<p>$$
L(x, y, \sigma) = G(x, y, \sigma)* I(x, y)
$$</p>
<p>Where:</p>
<ul>
<li>L is the blurred image</li>
<li>G is a Gaussian blur operator</li>
<li>I is the input image</li>
<li>σ acts as a scale parameter ( Higher value results in more blur)</li>
</ul>
<p>So, we first take the original image and blur it using a Gaussian convolution. What follows is a sequence of further convolutions with increasing standard deviation(σ). Images of same size (with different blur levels) are called an Octave. Then, we downsize the original image by a factor of 2. This starts another row of convolutions. We repeat this process until the pictures are too small to proceed.</p>

    <img src="https://i.imgur.com/MkAyEIZ.png"  class="center"  />


<center>  <figcaption>Figure 1 : (top left) A grey-level image and the scale-space representations computed at scale levels t = 1,8 and 64.  
Image from:  
Encyclopedia of Computer Science and Engineering (Benjamin Wah, ed), John Wiley and Sons, Volume IV, pages 2495–2504, Hoboken, New Jersey, 2009.</figcaption> </center>
<p>Now we have constructed a scale space. We do this to handle the multiscale nature of real-world objects.</p>
<h3 id="laplacian-of-gaussian-log-approximations">Laplacian of Gaussian (LoG) approximations</h3>
<p>Since we are finding the most stable image features we consider Lapcian of Gaussian. In detailed experimental comparisons, Mikolajczyk (2002) found that maxima and minima of Laplacian of Gaussian produce the most stable image features compared to a range of other possible image functions, such as the gradient, Hessian, or Harris corner function.</p>
<p>The problem that occurs here is that calculating all those second order derivatives is computationally intensive so we use Difference of Gaussians which is an approximation of LoG. Difference of Gaussian is obtained as the difference of Gaussian blurring of an image with two different σ and is given by:</p>
<p>$$
D(x,y,\sigma) = (G(x,y,k\sigma) - G(x,y,\sigma)) * I(x,y)
$$
$$
D(x,y,\sigma) = L(x,y,k\sigma) - L(x,y,\sigma)
$$
It is represented in below image:</p>

    <img src="https://i.imgur.com/Xwkf4e4.png"  class="center"  />


<center>  <figcaption>Figure 2 : For each octave of scale space, the initial image is repeatedly convolved with Gaussians to produce the set of scale space images shown on the left. Adjacent Gaussian images are subtracted to produce the difference-of-Gaussian images on the right. After each octave, the Gaussian image is down-sampled by a factor of 2, and the process repeated.  
Image from:  
Lowe, David G. "Distinctive image features from scale-invariant keypoints." International journal of computer vision 60.2 (2004): 91-110.</figcaption> </center>
<p>This is done for all octaves. The resulting images are an approximation of scale invariant laplacian of gaussian (which produces stable image keypoints).</p>
<h2 id="2-keypoint-localization">2. Keypoint Localization</h2>
<p>Now that we have found potential keypoints, we have to refine it further for more accurate results.</p>
<h3 id="local-maximaminima-detection">Local maxima/minima detection</h3>
<p>The first step is to locate the maxima and minima of Difference of Gaussian(DoG) images. Each pixel in the DoG images is compared to its 8 neighbours at the same scale, plus the 9 corresponding neighbours at neighbouring scales. If the pixel is a local maximum or minimum, it is selected as a candidate keypoint.</p>

    <img src="https://i.imgur.com/Tu8JHjC.png"  class="center"  />


<center>  <figcaption>Figure 3 : Maxima and minima of the difference-of-Gaussian images are detected by comparing a pixel (marked with X) to its 26   neighbours in 3 × 3 regions at the current and adjacent scales (marked with circles).  
Image from:  
Lowe, David G. "Distinctive image features from scale-invariant keypoints." International journal of computer vision 60.2 (2004): 91-110.</figcaption> </center>
<p>Once a keypoint candidate has been found by comparing a pixel to its neighbors, the next step is to refine the location of these feature points to sub-pixel accuracy whilst simultaneously removing any poor features.</p>
<h3 id="sub-pixel-refinement">Sub-Pixel Refinement</h3>
<p>The sub-pixel localization proceeds by fitting a 3D quadratic function to the local sample points to determine the interpolated location of the maximum. This approach uses the Taylor expansion (up to the quadratic terms) of the scale-space function, D(x, y, σ), shifted so that the origin is at the sample point:</p>
<p>$$
D(x) = D + \frac{\partial D^T}{\partial x} x + \frac{1}{2}x^T \frac{\partial^2 D}{\partial x^2}x
$$</p>
<p>The location of the extremum, \(x̂\) , is determined by taking the derivative of this function with respect to x and setting it to zero, giving:</p>
<p>$$
\hat{x} = - \frac{\partial^2 D^{-1}}{\partial x^2}\frac{\partial D}{\partial x}
$$</p>
<p>On solving, we&rsquo;ll get subpixel key point locations. Now we need to remove keypoints which have low contrast or lie along the edge as they are not useful to us.</p>
<h3 id="removing-low-contrast-keypoints">Removing Low Contrast Keypoints</h3>
<p>The function value at the extremum, D(x̂), is useful for rejecting unstable extrema with low contrast. This can be obtained by substituting extremum x̂ into the Taylor Expansion (upto quadratic terms) as given above, giving:
$$
D(\hat{x} ) = D + \frac{1}{2} \frac{\partial D^T}{\partial x}\hat{x}
$$</p>
<h3 id="removing-edge-responses">Removing Edge Responses</h3>
<p>This is achieved by using a 2x2 Hessian matrix (H) to compute the principal curvature. A poorly defined peak in the difference-of-Gaussian function will have a large principal curvature across the edge but a small one in the perpendicular direction.</p>

    <img src="https://i.imgur.com/qrmNemy.png"  class="center"  />


<center>  <figcaption>Image source: https://courses.cs.washington.edu/courses/cse455/10au/notes/SIFT.ppt</figcaption> </center>
<p>So from the calculation from hessian matrix we reject the flats and edges and keep the corner keypoints.</p>
<h2 id="3-keypoint-orientation-assignment">3. Keypoint Orientation Assignment</h2>
<p>To determine the keypoint orientation, a gradient orientation histogram is computed in the neighbourhood of the keypoint.</p>
<p>The magnitude and orientation is calculated for all pixels around the keypoint. Then, a histogram with 36 bins covering 360 degrees is created.</p>
<p>$$
m(x,y) = \sqrt{(L(x+1,y) -  L(x-1,y))^2 + (L(x,y+1) - L(x,y-1))^2 }
$$</p>
<p>$$
\theta(x,y) = \tan^{-1} {\frac{L(x,y+1) - L(x,y-1)}{L(x+1,y) -  L(x-1,y)}}
$$</p>
<p>\(m(x,y)\) is magnitude and \(\theta(x,y)\) is the orientation of the pixel at x,y location.
Each sample added to the histogram is weighted by its gradient magnitude and by a Gaussian-weighted circular window with a σ that is 1.5 times that of the scale of the keypoint.</p>
<p>Each sample added to the histogram is weighted by its gradient magnitude and by a Gaussian-weighted circular window with a σ that is 1.5 times that of the scale of the keypoint.</p>

    <img src="https://i.imgur.com/m1TJk5R.jpg"  class="center"  />


<center>  <figcaption>Image source: http://aishack.in/tutorials/sift-scale-invariant-feature-transform-keypoint-orientation</figcaption> </center>
<p>When it is done for all the pixels around the keypoint, the histogram will have a peak at some point. And any peaks above 80% of the highest peak are converted into a new keypoint. This new keypoint has the same location and scale as the original. But it&rsquo;s orientation is equal to the other peak.</p>
<h2 id="4-keypoint-descriptor">4. Keypoint Descriptor</h2>
<p>Once a keypoint orientation has been selected, the feature descriptor is computed as a set of orientation histograms.</p>
<p>To do this, a 16x16 window around the keypoint is taken. It is divided into 16 sub-blocks of 4x4 size.</p>
<p>Within each 4x4 window, gradient magnitudes and orientations are calculated. These orientations are put into an 8 bin histogram.</p>
<p>Histograms contain 8 bins each, and each descriptor contains an array of 4 histograms around the keypoint. This leads to a SIFT feature vector with 4 × 4 × 8 = 128 elements.</p>

    <img src="https://i.imgur.com/Nm02XiB.jpg"  class="center"  />


<p>This feature vector introduces a few complications.</p>
<ul>
<li>Since we use gradient orientations, if you rotate the image, all gradient orientations also change. To solve this we subtract the keypoint&rsquo;s rotation from each orientation. Thus each gradient orientation is relative to the keypoint&rsquo;s orientation.</li>
<li>We also normalize the vector to enhance invariance to changes in illumination. So we threshold the values in the feature vector to each be no larger than 0.2 (i.e. if value larger than 0.2 then it is set to 0.2).</li>
</ul>
<h1 id="sift-implementation">SIFT Implementation</h1>
<p>In this section we will be performing object detection using SIFT with the help of opencv library in python.</p>
<p>Now before starting object detection let&rsquo;s first see the keypoint detection.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> cv2
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

train_img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;train.jpg&#39;</span>)   <span style="color:#75715e"># train image</span>
query_img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;query.jpg&#39;</span>)   <span style="color:#75715e"># query/test image</span>

<span style="color:#75715e"># Turn Images to grayscale</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">to_gray</span>(color_img):
    gray <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(color_img, cv2<span style="color:#f92672">.</span>COLOR_BGR2GRAY)
    <span style="color:#66d9ef">return</span> gray

train_img_gray <span style="color:#f92672">=</span> to_gray(train_img)
query_img_gray <span style="color:#f92672">=</span> to_gray(query_img)

<span style="color:#75715e"># Initialise SIFT detector</span>
sift <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>xfeatures2d<span style="color:#f92672">.</span>SIFT_create()

<span style="color:#75715e"># Generate SIFT keypoints and descriptors</span>
train_kp, train_desc <span style="color:#f92672">=</span> sift<span style="color:#f92672">.</span>detectAndCompute(train_img_gray, None)
query_kp, query_desc <span style="color:#f92672">=</span> sift<span style="color:#f92672">.</span>detectAndCompute(query_img_gray, None)

plt<span style="color:#f92672">.</span>figure(<span style="color:#ae81ff">1</span>)
plt<span style="color:#f92672">.</span>imshow((cv2<span style="color:#f92672">.</span>drawKeypoints(train_img_gray, train_kp, train_img<span style="color:#f92672">.</span>copy())))
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Train Image Keypoints&#39;</span>)

plt<span style="color:#f92672">.</span>figure(<span style="color:#ae81ff">2</span>)
plt<span style="color:#f92672">.</span>imshow((cv2<span style="color:#f92672">.</span>drawKeypoints(query_img_gray, query_kp, query_img<span style="color:#f92672">.</span>copy())))
plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Query Image Keypoints&#39;</span>)

plt<span style="color:#f92672">.</span>show()

</code></pre></div><p>Here I took pictures of Taj Mahal from different viewpoints for train and query image.</p>

    <img src="https://i.imgur.com/hItFfSK.png"  class="center"  />



    <img src="https://i.imgur.com/RhAc0mr.png"  class="center"  />



    <img src="https://i.imgur.com/VUSkJJ2.png"  class="center"  />



    <img src="https://i.imgur.com/i2o2x1b.png"  class="center"  />


<p>As you can see from the above code that the following function :</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Initialise SIFT detector</span>
sift <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>xfeatures2d<span style="color:#f92672">.</span>SIFT_create()

<span style="color:#75715e"># Generate SIFT keypoints and descriptors</span>
train_kp, train_desc <span style="color:#f92672">=</span> sift<span style="color:#f92672">.</span>detectAndCompute(train_img_gray, None)
query_kp, query_desc <span style="color:#f92672">=</span> sift<span style="color:#f92672">.</span>detectAndCompute(query_img_gray, None)

</code></pre></div><p>is the one that does the computations for the SIFT algorithm and returns keypoints and descriptors of the image.</p>
<p>We can use the keypoints and descriptors for feature matching between two objects and finally find object in the query image.</p>
<p>Now we move onto feature matching part. We will match features in one image with others.</p>
<p>For feature matching we are using Brute-Force matcher provided by OpenCV. You can also use FLANN Matcher in OpenCV as I will use in further section of the tutorial.</p>
<p>Brute-Force matcher takes the descriptor of one feature in first set and is matched with all other features in second set using some distance calculation. And the closest one is returned.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># create a BFMatcher object which will match up the SIFT features</span>
bf <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>BFMatcher(cv2<span style="color:#f92672">.</span>NORM_L2, crossCheck<span style="color:#f92672">=</span>True)

matches <span style="color:#f92672">=</span> bf<span style="color:#f92672">.</span>match(train_desc, query_desc)

<span style="color:#75715e"># Sort the matches in the order of their distance.</span>
matches <span style="color:#f92672">=</span> sorted(matches, key <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x:x<span style="color:#f92672">.</span>distance)

<span style="color:#75715e"># draw the top N matches</span>
N_MATCHES <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>

match_img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>drawMatches(
    train_img, train_kp,
    query_img, query_kp,
    matches[:N_MATCHES], query_img<span style="color:#f92672">.</span>copy(), flags<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)

plt<span style="color:#f92672">.</span>figure(<span style="color:#ae81ff">3</span>)
plt<span style="color:#f92672">.</span>imshow(match_img)
plt<span style="color:#f92672">.</span>show()

</code></pre></div><p>The visualization of how the SIFT features match up each other across the two images is as follow:</p>

    <img src="https://i.imgur.com/D4KsaP4.png"  class="center"  />


<p>So till now we have found the keypoints and descriptors for the train and query images and then matched top keypoints and visualized it. But this is still not sufficient to find the object.</p>
<p>For that, we can use a function cv2.findHomography(). If we pass the set of points from both the images, it will find the perpective transformation of that object. Then we can use cv2.perspectiveTransform() to find the object. It needs atleast four correct points to find the transformation.</p>
<p>So now we will use a train image and then try to detect it in the real-time.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> cv2
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#75715e"># Threshold</span>
MIN_MATCH_COUNT<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>

<span style="color:#75715e"># Initiate SIFT detector</span>
sift<span style="color:#f92672">=</span>cv2<span style="color:#f92672">.</span>xfeatures2d<span style="color:#f92672">.</span>SIFT_create()


<span style="color:#75715e"># Create the Flann Matcher object</span>
FLANN_INDEX_KDITREE<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
flannParam<span style="color:#f92672">=</span>dict(algorithm<span style="color:#f92672">=</span>FLANN_INDEX_KDITREE,tree<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
flann<span style="color:#f92672">=</span>cv2<span style="color:#f92672">.</span>FlannBasedMatcher(flannParam,{})


train_img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#34;obama1.jpg&#34;</span>,<span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># train image</span>
<span style="color:#75715e"># find the keypoints and descriptors with SIFT</span>
kp1,desc1 <span style="color:#f92672">=</span> sift<span style="color:#f92672">.</span>detectAndCompute(train_img,None)
<span style="color:#75715e"># draw keypoints of the train image</span>
train_img_kp<span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>drawKeypoints(train_img,kp1,None,(<span style="color:#ae81ff">255</span>,<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>),<span style="color:#ae81ff">4</span>)
<span style="color:#75715e"># show the train image keypoints</span>
plt<span style="color:#f92672">.</span>imshow(train_img_kp)    
plt<span style="color:#f92672">.</span>show()

<span style="color:#75715e"># start capturing video</span>
cap <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>VideoCapture(<span style="color:#ae81ff">0</span>)

<span style="color:#66d9ef">while</span> True:
    ret, frame <span style="color:#f92672">=</span> cap<span style="color:#f92672">.</span>read()
    <span style="color:#75715e"># turn the frame captured into grayscale.</span>
    gray <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>cvtColor(frame,cv2<span style="color:#f92672">.</span>COLOR_BGR2GRAY)
    <span style="color:#75715e"># find the keypoints and descriptors with SIFT  of the frame captured.</span>
    kp2, desc2 <span style="color:#f92672">=</span> sift<span style="color:#f92672">.</span>detectAndCompute(gray,None)   

    <span style="color:#75715e"># Obtain matches using K-Nearest Neighbor Method.</span>
    <span style="color:#75715e">#&#39;matches&#39; is the number of similar matches found in both images.</span>
    matches<span style="color:#f92672">=</span>flann<span style="color:#f92672">.</span>knnMatch(desc2,desc1,k<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)

    <span style="color:#75715e"># store all the good matches as per Lowe&#39;s ratio test.</span>
    goodMatch<span style="color:#f92672">=</span>[]
    <span style="color:#66d9ef">for</span> m,n <span style="color:#f92672">in</span> matches:
        <span style="color:#66d9ef">if</span>(m<span style="color:#f92672">.</span>distance<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0.75</span><span style="color:#f92672">*</span>n<span style="color:#f92672">.</span>distance):
            goodMatch<span style="color:#f92672">.</span>append(m)

    <span style="color:#75715e"># If enough matches are found, we extract the locations of matched keypoints in both the images.</span>
    <span style="color:#75715e"># They are passed to find the perpective transformation.</span>
    <span style="color:#75715e"># Then we are able to locate our object.</span>
    <span style="color:#66d9ef">if</span>(len(goodMatch)<span style="color:#f92672">&gt;</span>MIN_MATCH_COUNT):
        tp<span style="color:#f92672">=</span>[]  <span style="color:#75715e"># src_pts</span>
        qp<span style="color:#f92672">=</span>[]  <span style="color:#75715e"># dst_pts</span>
        <span style="color:#66d9ef">for</span> m <span style="color:#f92672">in</span> goodMatch:
            tp<span style="color:#f92672">.</span>append(kp1[m<span style="color:#f92672">.</span>trainIdx]<span style="color:#f92672">.</span>pt)
            qp<span style="color:#f92672">.</span>append(kp2[m<span style="color:#f92672">.</span>queryIdx]<span style="color:#f92672">.</span>pt)
        tp,qp<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32((tp,qp))

        H,status<span style="color:#f92672">=</span>cv2<span style="color:#f92672">.</span>findHomography(tp,qp,cv2<span style="color:#f92672">.</span>RANSAC,<span style="color:#ae81ff">3.0</span>)


        h,w <span style="color:#f92672">=</span> train_img<span style="color:#f92672">.</span>shape
        train_outline<span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>float32([[[<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">0</span>],[<span style="color:#ae81ff">0</span>,h<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>],[w<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,h<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>],[w<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">0</span>]]])
        query_outline <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>perspectiveTransform(train_outline,H)

        cv2<span style="color:#f92672">.</span>polylines(frame,[np<span style="color:#f92672">.</span>int32(query_outline)],True,(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">255</span>,<span style="color:#ae81ff">0</span>),<span style="color:#ae81ff">5</span>)
        cv2<span style="color:#f92672">.</span>putText(frame,<span style="color:#e6db74">&#39;Object Found&#39;</span>,(<span style="color:#ae81ff">50</span>,<span style="color:#ae81ff">50</span>), cv2<span style="color:#f92672">.</span>FONT_HERSHEY_COMPLEX, <span style="color:#ae81ff">2</span> ,(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">255</span>,<span style="color:#ae81ff">0</span>), <span style="color:#ae81ff">2</span>)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Match Found-&#34;</span>)
        <span style="color:#66d9ef">print</span>(len(goodMatch),MIN_MATCH_COUNT)

    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Not Enough match found-&#34;</span>)
        <span style="color:#66d9ef">print</span>(len(goodMatch),MIN_MATCH_COUNT)
    cv2<span style="color:#f92672">.</span>imshow(<span style="color:#e6db74">&#39;result&#39;</span>,frame)

    <span style="color:#66d9ef">if</span> cv2<span style="color:#f92672">.</span>waitKey(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">==</span> <span style="color:#ae81ff">13</span>:
        <span style="color:#66d9ef">break</span>
cap<span style="color:#f92672">.</span>release()
cv2<span style="color:#f92672">.</span>destroyAllWindows()

</code></pre></div><p>Result :</p>

    <img src="https://i.imgur.com/QmcJQ1P.png"  class="center"  />



    <img src="https://i.imgur.com/w7fIMHs.png"  class="center"  />


<p>To see the full code for this post check out this  <a href="https://github.com/Eklavya42/SIFT-Scale-Invariant-Feature-Transform">repository</a></p>
<h2 id="disadvantages-of-sift-algorithm">Disadvantages of SIFT algorithm</h2>
<ul>
<li>SIFT uses 128 dimensional feature vectors which are big and computational cost of SIFT due to this rises.</li>
<li>SIFT continues to be a good detector when the images that are to be matches are nearly identical but even a relatively small change will produce a big drop in matching keypoints.</li>
<li>SIFT cannot find too many points in the image that are resistant to scale, rotation and distortion if the original image is out of focus (blurred). Thus, it does not work well if the images are blurred.</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Face Detection using Histogram of Oriented Gradients</title>
            <link>/posts/2019/03/face-detection-using-histogram-of-oriented-gradients/</link>
            <pubDate>Mon, 11 Mar 2019 17:15:12 +0530</pubDate>
            
            <guid>/posts/2019/03/face-detection-using-histogram-of-oriented-gradients/</guid>
            <description>What is Histogram of Oriented Gradients (HOG)? Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005. Histogram of Oriented Gradients (HOG) is a feature descriptor used in image processing, mainly for object detection. A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information from it.
The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions.</description>
            <content type="html"><![CDATA[<h2 id="what-is-histogram-of-oriented-gradients-hog">What is Histogram of Oriented Gradients (HOG)?</h2>
<p>Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005. Histogram of Oriented Gradients (HOG) is a feature descriptor used in image processing, mainly for object detection. A feature descriptor is a representation of an image or an image patch that simplifies the image by extracting useful information from it.</p>
<p>The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions. The x and y derivatives of an image (Gradients) are useful because the magnitude of gradients is large around edges and corners due to abrupt change in intensity and we know that edges and corners pack in a lot more information about object shape than flat regions. So, the histograms of directions of gradients are used as features in this descriptor.</p>
<h2 id="workflow-of-face-detection-using-hog">Workflow of face detection using HOG</h2>
<p>Now that we know basic principle of Histogram of Oriented Gradients we will be moving into how we calculate the histograms and how these feature vectors, that are obtained from the HOG descriptor, are used by the classifier such a SVM to detect the concerned object.</p>

    <img src="https://i.imgur.com/5qac3GF.png"  class="center"  />


<center>  <figcaption>Steps for Object Detection with HOG</figcaption> </center>
<h2 id="how-histogram-of-oriented-gradientshog-works">How Histogram of Oriented Gradients(HOG) Works?</h2>
<h3 id="pre-processing">Pre-processing</h3>
<p>Preprocessing of image involves normalising the image but it is entirely optional. It is used to improve performance of the HOG descriptor. Since, here we are building a simple descriptor we don&rsquo;t use any normalisation in preprocessing.</p>
<h3 id="computing-gradient">Computing Gradient</h3>
<p>The first actual step in the HOG descriptor is to compute the image gradient in both the x and y direction.</p>
<p>Let us take an example. Say the pixel Q has values surrounding it as shown below:</p>

    <img src="https://i.imgur.com/sdA8be8.png"  class="center"  />


<p>We can calculate the Gradient magnitude for Q in x and y direction as follow:
$$
G_x = 100 -50 =50
$$</p>
<p>$$
G_y = 120 -70 =50
$$</p>
<p>We can get the magnitude of the gradient as:</p>
<p>$$
G= \sqrt{(G_x)^2 + (G_y)^2} = 70.7
$$
And the direction of the gradient as :</p>
<p>$$
\theta = arctan({\frac {G_y} {G_x}}) = 45^\circ
$$</p>
<h3 id="compute-histogram-of-gradients-in-88-cells">Compute Histogram of Gradients in 8×8 cells</h3>
<ul>
<li>
<p>The image is divided into 8×8 cell blocks and a histogram of gradients is calculated for each 8×8 cell block.</p>
</li>
<li>
<p>The histogram is essentially a vector of 9 buckets ( numbers ) corresponding to angles from \(0^ \circ\) to  \(180^ \circ\) (\(20^ \circ\) increments.)</p>
</li>
<li>
<p>The values of these 64 cells (8X8) are binned and cumulatively added into these 9 buckets.</p>
</li>
<li>
<p>This essentially reduces 64 values into 9 values.</p>
</li>
</ul>
<p>A great illustration of this is shown on  <a href="https://www.learnopencv.com/histogram-of-oriented-gradients/">learnopencv</a>. The following figure shows how it is done. The blue pixel encircled has an angle of \(80^ \circ\) and magnitude of 2. So it adds 2 to the 5th bin. The gradient at the pixel encircled using red has an angle of \(10^ \circ\) and magnitude of 4. Since \(0^ \circ\) is half way between \(0^ \circ\) and \(20^ \circ\), the vote by the pixel splits evenly into the two bins.</p>

    <img src="https://i.imgur.com/6zN14kq.png"  class="center"  />


<center>  <figcaption>Illustration of splitting of gradient magnitude according to gradient direction (Image source: https://www.learnopencv.com/histogram-of-oriented-gradients)</figcaption> </center>
<h3 id="block-normalisation">Block Normalisation</h3>
<p>After the creation of histogram of oriented gradients we need to something else too. Gradient is sensitive to overall lighting. If we say divide/multiply pixel values by some constant in order to make it lighter/ darker the gradient magnitude will change and so will histogram values. We want that histogram values be independent of lighting. Normalisation is done on the histogram vector v within a block. One of the following norms could be used:</p>
<ul>
<li>L1 norm</li>
<li>L2 norm</li>
<li>L2-Hys(Lowe-style clipped L2 norm)</li>
</ul>
<p>Now, we could simply normalise the 9×1 histogram vector but it is better to normalise a bigger sized block of 16×16. A 16×16 block has 4 histograms (8×8 cell results to one histogram) which can be concatenated to form a 36 x 1 element vector and normalised. The 16×16 window then moves by 8 pixels and a normalised 36×1 vector is calculated over this window and the process is repeated for the image.</p>
<h3 id="calculate-hog-descriptor-vector">Calculate HOG Descriptor vector</h3>
<ul>
<li>To calculate the final feature vector for the entire image patch, the 36×1 vectors are concatenated into one giant vector.</li>
<li>So, say if there was an input picture of size 64×64 then the 16×16 block has 7 positions horizontally and 7 position vertically.</li>
<li>In one 16×16 block we have 4 histograms which after normalisation concatenate to form a 36×1 vector.</li>
<li>This block moves 7 positions horizontally and vertically totalling it to 7×7 = 49 positions.</li>
<li>So when we concatenate them all into one giant vector we obtain a 36×49 = 1764 dimensional vector.</li>
</ul>
<p>This vector is now used to train classifiers such as SVM and then do object detection.</p>
<h3 id="visualization-of-hog-features">Visualization of HOG features</h3>
<p>Here is a snippet to visualise HOG features of an Image provided in  <a href="https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html">Scikit-Image&rsquo;s docs</a>  to visualize HOG features.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt

<span style="color:#f92672">from</span> skimage.feature <span style="color:#f92672">import</span> hog
<span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> data, exposure


image <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>astronaut()

fd, hog_image <span style="color:#f92672">=</span> hog(image, orientations<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, pixels_per_cell<span style="color:#f92672">=</span>(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>),
                    cells_per_block<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), visualize<span style="color:#f92672">=</span>True, multichannel<span style="color:#f92672">=</span>True)

fig, (ax1, ax2) <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>), sharex<span style="color:#f92672">=</span>True, sharey<span style="color:#f92672">=</span>True)

ax1<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
ax1<span style="color:#f92672">.</span>imshow(image, cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>gray)
ax1<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Input image&#39;</span>)

<span style="color:#75715e"># Rescale histogram for better display</span>
hog_image_rescaled <span style="color:#f92672">=</span> exposure<span style="color:#f92672">.</span>rescale_intensity(hog_image, in_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>))

ax2<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
ax2<span style="color:#f92672">.</span>imshow(hog_image_rescaled, cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>gray)
ax2<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#39;Histogram of Oriented Gradients&#39;</span>)
plt<span style="color:#f92672">.</span>show()


</code></pre></div><p>Output as follow :</p>

    <img src="https://i.imgur.com/EnNkv49.png"  class="center"  />


<center>  <figcaption>Visualisation of HOG features of image of the astronaut Eileen Collins.</figcaption> </center>
<h2 id="implementation">Implementation</h2>
<p>In this tutorial we will be performing a simple Face Detection using HOG features.<br>
We need to first train the classifier in order to do face detection so first we will need to have training set for the classifier.</p>
<h3 id="training-set">Training Set</h3>
<ul>
<li>Positive training samples</li>
</ul>
<p>Labelled Faces in the Wild dataset provided by Scikit-Learn consists of variety of faces which is perfect for our positive set.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> fetch_lfw_people
faces <span style="color:#f92672">=</span> fetch_lfw_people()
positive_patches <span style="color:#f92672">=</span> faces<span style="color:#f92672">.</span>images

</code></pre></div><ul>
<li>Negative training samples</li>
</ul>
<p>For Negative set we need images without face on them. Scikit-Image offers images which can be used in this case. To increase the size of negative set we extract patches of image at different scale using Patch Extractor from Scikit-Learn.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> data, transform
<span style="color:#f92672">from</span> sklearn.feature_extraction.image <span style="color:#f92672">import</span> PatchExtractor

imgs_to_use <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;camera&#39;</span>, <span style="color:#e6db74">&#39;text&#39;</span>, <span style="color:#e6db74">&#39;coins&#39;</span>, <span style="color:#e6db74">&#39;moon&#39;</span>,
               <span style="color:#e6db74">&#39;page&#39;</span>, <span style="color:#e6db74">&#39;clock&#39;</span>, <span style="color:#e6db74">&#39;immunohistochemistry&#39;</span>,
               <span style="color:#e6db74">&#39;chelsea&#39;</span>, <span style="color:#e6db74">&#39;coffee&#39;</span>, <span style="color:#e6db74">&#39;hubble_deep_field&#39;</span>]
images <span style="color:#f92672">=</span> [color<span style="color:#f92672">.</span>rgb2gray(getattr(data, name)())
          <span style="color:#66d9ef">for</span> name <span style="color:#f92672">in</span> imgs_to_use]


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_patches</span>(img, N, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, patch_size<span style="color:#f92672">=</span>positive_patches[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>shape):
    extracted_patch_size <span style="color:#f92672">=</span> tuple((scale <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>array(patch_size))<span style="color:#f92672">.</span>astype(int))
    extractor <span style="color:#f92672">=</span> PatchExtractor(patch_size<span style="color:#f92672">=</span>extracted_patch_size,
                               max_patches<span style="color:#f92672">=</span>N, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    patches <span style="color:#f92672">=</span> extractor<span style="color:#f92672">.</span>transform(img[np<span style="color:#f92672">.</span>newaxis])
    <span style="color:#66d9ef">if</span> scale <span style="color:#f92672">!=</span> <span style="color:#ae81ff">1</span>:
        patches <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([transform<span style="color:#f92672">.</span>resize(patch, patch_size)
                            <span style="color:#66d9ef">for</span> patch <span style="color:#f92672">in</span> patches])
    <span style="color:#66d9ef">return</span> patches

negative_patches <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>vstack([extract_patches(im, <span style="color:#ae81ff">1000</span>, scale)
                              <span style="color:#66d9ef">for</span> im <span style="color:#f92672">in</span> images <span style="color:#66d9ef">for</span> scale <span style="color:#f92672">in</span> [<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>]])

</code></pre></div><h3 id="extract-hog-features">Extract HOG Features</h3>
<p>Scikit-Image&rsquo;s feature module offers a function skimage.feature.hog which extracts Histogram of Oriented Gradients (HOG) features for a given image. we combine the positive and negative set and compute the HOG features</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> feature   
<span style="color:#f92672">from</span> itertools <span style="color:#f92672">import</span> chain

X_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([feature<span style="color:#f92672">.</span>hog(im)
                    <span style="color:#66d9ef">for</span> im <span style="color:#f92672">in</span> chain(positive_patches,
                                    negative_patches)])
y_train <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])
y_train[:positive_patches<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>

</code></pre></div><h3 id="training-a-svm-classifier">Training a SVM classifier</h3>
<p>We will use Scikit-Learn&rsquo;s LinearSVC with a grid search over a few choices of the C parameter:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> sklearn.svm <span style="color:#f92672">import</span> LinearSVC
<span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> GridSearchCV

grid <span style="color:#f92672">=</span> GridSearchCV(LinearSVC(dual<span style="color:#f92672">=</span>False), {<span style="color:#e6db74">&#39;C&#39;</span>: [<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">4.0</span>, <span style="color:#ae81ff">8.0</span>]},cv<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
grid<span style="color:#f92672">.</span>fit(X_train, y_train)
grid<span style="color:#f92672">.</span>best_score_

</code></pre></div><p>We will take the best estimator and then build a model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">model <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>best_estimator_
model<span style="color:#f92672">.</span>fit(X_train, y_train)

</code></pre></div><h3 id="testing-on-a-new-image">Testing on a new image</h3>
<p>Now that we have built the Model we can test it on a new image to see how it detects the faces.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> io

img <span style="color:#f92672">=</span> io<span style="color:#f92672">.</span>imread(<span style="color:#e6db74">&#39;testpic.jpg&#39;</span>,as_gray<span style="color:#f92672">=</span>True)
img <span style="color:#f92672">=</span> skimage<span style="color:#f92672">.</span>transform<span style="color:#f92672">.</span>rescale(img, <span style="color:#ae81ff">0.5</span>)
indices, patches <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>sliding_window(img))
patches_hog <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([feature<span style="color:#f92672">.</span>hog(patch) <span style="color:#66d9ef">for</span> patch <span style="color:#f92672">in</span> patches])
labels <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(patches_hog)

</code></pre></div><p>We are detecting the face by using a sliding window which goes over the image patches. Then we find the HOG feature of these patches. Finally, we run it through the classification model that we build and predict the face in the image. The image below is one of the test images. We can see that the classifier detected patches and most of them overlap the face in the image.</p>

    <img src="https://i.imgur.com/SkKwtRa.png"  class="center"  />


<p>To see the full code for this post check out this  <a href="https://github.com/Eklavya42/ComputerVision/tree/master/HOG">repository</a></p>
]]></content>
        </item>
        
        <item>
            <title>Hopfield Network</title>
            <link>/posts/2019/01/hopfield-network/</link>
            <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
            
            <guid>/posts/2019/01/hopfield-network/</guid>
            <description>Hopfield Network is a recurrent neural network with bipolar threshold neurons. In this article, we will go through in depth along with an implementation. Before going into Hopfield network, we will revise basic ideas like Neural network and perceptron.
A neural network is a mathematical model or computational model inspired by biological neural networks. It consists of an interconnected group of artiﬁcial neurons. The sructure and functioning of the central nervous system constituing neurons, axons, dentrites and syanpses which make up the processing parts of the biological neural networks were the original inspiration that led to the developement of computational models of neural networks.</description>
            <content type="html"><![CDATA[<p><strong>Hopfield Network</strong>  is a recurrent neural network with bipolar threshold neurons. In this article, we will go through in depth along with an implementation. Before going into Hopfield network, we will revise basic ideas like Neural network and perceptron.</p>
<p>A  <strong>neural network</strong>  is a mathematical model or computational model inspired by biological neural networks. It consists of an interconnected group of artiﬁcial neurons. The sructure and functioning of the central nervous system constituing neurons, axons, dentrites and syanpses which make up the processing parts of the biological neural networks were the original inspiration that led to the developement of computational models of neural networks.</p>
<p>The ﬁrst computational model of a  <strong>neuron</strong>  was presented in 1943 by  <strong>W. Mc Culloch</strong>  and  <strong>W. Pitts</strong>. They called this model threshold logic.The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.</p>

    <img src="https://i.imgur.com/Qi53ISi.png"  alt="Neuron"  class="center"  />


<p>In 1958,  <strong>Rossenblatt</strong>  conceived the  <strong>Perceptron</strong>, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtractio. His work had big repercussion but in 1969 a violent critic by Minsky and Papert was published.</p>
<p>The work on neural network was slow down but  <strong>John Hopﬁeld</strong>  convinced of the power of neural network came out with his model in 1982 and boost research in this ﬁeld. Hopﬁeld Network is a particular case of Neural Network. It is based on physics, inspired by spin system.</p>

    <img src="https://i.imgur.com/MybBfsm.png"  alt="Neural network"  class="center"  />


<h2 id="the-network">The Network</h2>
<p><strong>Hopfield Network</strong>  is a  <strong>recurrent neural network</strong>  with  <strong>bipolar threshold neurons</strong>. Hopﬁeld network consists of a set of interconnected neurons which update their activation values asynchronously. The activation values are binary, usually {-1,1}. The update of a unit depends on the other units of the network and on itself.</p>
<p>A neuron in Hopfield model is  <strong>binary</strong>  and defined by the  <strong>standard McCulloch-Pitts model</strong>  of a neuron:</p>
<p>\[n_i (t+1)= \theta(\sum_{j}w_{ij} n_j (t) - \mu_i)  \tag{1}\]</p>
<p>where n<sub>i</sub>(t+1) is the i<sup>th</sup> neuron at time t+1, n<sub>j</sub>(t) is the j<sup>th</sup> neuron at time t, w<sub>ij</sub> is the weight matrix called <em>synaptic weights</em> , θ is the step function and μ is the bias. In the Hopfield model the neurons have a binary output taking values -1 and 1. Thus the model has following form:</p>
<p>\[S_i(t+1) = sgn(\sum_{j}w_{ij} S_j(t) - \vartheta_i) \tag{2} \]</p>
<p>where S<sub>i</sub> and n<sub>i</sub> are related through the formula: S<sub>i</sub> = 2n<sub>i</sub> - 1 (Since n<sub>i</sub> ϵ [0,1] and S<sub>i</sub> ϵ [-1,1] ).  ϑ<sub>i</sub> is the threshold, so if the input is above the threshold it will fire 1. So here S represents the neurons which were represented as n in equation 1. The <em>sgn</em> sign here is the signum function which is described as follow:
$$
sgn(x) =
\begin{cases}
-1 &amp; \text{if $x$ &lt; 0,}\\\<br>
0 &amp; \text{if $x$ = 0,}\\\<br>
1 &amp; \text{if $x$ &gt; 0}
\end{cases}
$$</p>
<p>For ease of analysis in this post we will drop the threshold (ϑ<sub>i </sub>= 0) as we will analyse mainly random patterns and thresholds are not useful in this context. In this case the model is written as:</p>
<p>$$
S_i(t+1) = sgn(\sum_{j}w_{ij} S_j(t) ) \tag{3}
$$</p>
<h2 id="training-the-network">Training the Network</h2>
<p>In this post we are looking at Auto-associative model of Hopfield Network. It can store useful information in memory and later it is able to reproduce this information from partially broken patterns.</p>
<p>For training procedure, it doesn’t require any iterations. It includes just an outer product between input vector and transposed input vector to fill the weighted matrix w<sub>ij</sub>  (or synaptic weights) and in case of many patterns it is as follow:</p>
<p>$$
w_{i,j} = \frac{1}{N} \sum_{\mu=1}^p \epsilon_i^\mu\epsilon_j^\mu \tag{4}
$$</p>
<p>where, ε is the pattern and <em>p</em> = total number of patterns.</p>
<p>The main advantage of Auto-associative network is that it is able to recover pattern from the memory using just a partial information about the pattern. There are two main approaches to this situation.<br>
We update the neurons as specified in equation 3:</p>
<ul>
<li><strong>Syncronously</strong>: Update all the neurons simultaneously at each time step;</li>
<li><strong>Asyncronously</strong>: At each time step, select at random, a unit i and update it.</li>
</ul>
<h2 id="implementation">Implementation</h2>
<p>Now that we have covered the basics lets start implementing Hopfield network.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

nb_patterns <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>   <span style="color:#75715e"># Number of patterns to learn</span>
pattern_width <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
pattern_height <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
max_iterations <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>

<span style="color:#75715e"># Define Patterns</span>
patterns <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([
   [<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1.</span>],   <span style="color:#75715e"># Letter D</span>
   [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1.</span>],    <span style="color:#75715e"># Letter J</span>
   [<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>],     <span style="color:#75715e"># Letter C</span>
   [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>],], <span style="color:#75715e"># Letter M</span>
   dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float)

</code></pre></div><p>So we import the necessary libraries and define the patterns we want the network to learn. Here, we are defining 4 patterns. We can visualise them with the help of the code below:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Show the patterns</span>
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, nb_patterns, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">10</span>))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(nb_patterns):
    ax[i]<span style="color:#f92672">.</span>matshow(patterns[i]<span style="color:#f92672">.</span>reshape((pattern_height, pattern_width)), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
    ax[i]<span style="color:#f92672">.</span>set_xticks([])
    ax[i]<span style="color:#f92672">.</span>set_yticks([])

</code></pre></div><p>Which gives the out put as :</p>

    <img src="https://i.imgur.com/u6T2YkH.png"  class="center"  />


<p>We now train the network by filling the weight matrix as defined in the equation 4</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Train the network</span>
W <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((pattern_width <span style="color:#f92672">*</span> pattern_height, pattern_width <span style="color:#f92672">*</span> pattern_height))

<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(pattern_width <span style="color:#f92672">*</span> pattern_height):
    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(pattern_width <span style="color:#f92672">*</span> pattern_height):
        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> j <span style="color:#f92672">or</span> W[i, j] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0.0</span>:
            <span style="color:#66d9ef">continue</span>

        w <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>

        <span style="color:#66d9ef">for</span> n <span style="color:#f92672">in</span> range(nb_patterns):
            w <span style="color:#f92672">+=</span> patterns[n, i] <span style="color:#f92672">*</span> patterns[n, j]

        W[i, j] <span style="color:#f92672">=</span> w <span style="color:#f92672">/</span> patterns<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]  
        W[j, i] <span style="color:#f92672">=</span> W[i, j]  

</code></pre></div><p>Now that we have trained the network. We will create a corrupted pattern to test on this network.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Test the Network</span>
<span style="color:#75715e"># Create a corrupted pattern S</span>
S <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(    [<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>],     
   dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float)

<span style="color:#75715e"># Show the corrupted pattern</span>
fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
ax<span style="color:#f92672">.</span>matshow(S<span style="color:#f92672">.</span>reshape((pattern_height, pattern_width)), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)

</code></pre></div><p>The corrupted pattern we take here simply edditing some bits in the pattern array of letter C. We can see the corrupted pattern as follow:</p>

    <img src="https://i.imgur.com/XROw3hO.png"  class="center"  />


<p>We pass the corrupted pattern through the network and it is updated as defined in the equation 3. Thus, after each iteration some update is applied to the corrupted matrix. We take the hamming distance of the corrupted pattern which is being updated every time and all the patterns. And then we decide the closest pattern in terms of least hamming distance.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">h <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((pattern_width <span style="color:#f92672">*</span> pattern_height))
<span style="color:#75715e">#Defining Hamming Distance matrix for seeing convergence</span>
hamming_distance <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros((max_iterations,nb_patterns))
<span style="color:#66d9ef">for</span> iteration <span style="color:#f92672">in</span> range(max_iterations):
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(pattern_width <span style="color:#f92672">*</span> pattern_height):
        i <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(pattern_width <span style="color:#f92672">*</span> pattern_height)
        h[i] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(pattern_width <span style="color:#f92672">*</span> pattern_height):
            h[i] <span style="color:#f92672">+=</span> W[i, j]<span style="color:#f92672">*</span>S[j]
        S <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(h<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(nb_patterns):
        hamming_distance[iteration, i] <span style="color:#f92672">=</span> ((patterns <span style="color:#f92672">-</span> S)[i]<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>sum()   

    fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots()
    ax<span style="color:#f92672">.</span>matshow(S<span style="color:#f92672">.</span>reshape((pattern_height, pattern_width)), cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
hamming_distance

</code></pre></div>
    <img src="https://i.imgur.com/dMnz7lH.png"  class="center"  />


<p>Here we see that the hamming distance between corrupted pattern and third pattern i.e. letter C has become 0 after few iterations thus correcting the corrupted pattern.</p>
<p>We can also see the plot for all the hamming distances below:</p>

    <img src="https://i.imgur.com/iI89O7y.png"  class="center"  />


]]></content>
        </item>
        
        <item>
            <title>Object Detection Techniques</title>
            <link>/posts/2018/04/object-detection-techniques/</link>
            <pubDate>Mon, 02 Apr 2018 00:00:00 +0000</pubDate>
            
            <guid>/posts/2018/04/object-detection-techniques/</guid>
            <description>In this article, we will understand what is object detection, why we need to do object detection and the basic idea behind various techniques used to solved this problem. We start with the basic techniques like Viola Jones face detector to some of the advanced techniques like Single Shot Detector. Some the the techniques are:
 Viola Jones face detector Object Detection using Histogram of Oriented Gradients (HOG) Features Scale-invariant feature transform (SIFT) AlexNet Region-based Convolutional Network (R-CNN) You Only Look Once (YOLO) Single Shot Detector (SSD)  What is Object Detection?</description>
            <content type="html"><![CDATA[<p>In this article, we will understand what is object detection, why we need to do object detection and the basic idea behind various techniques used to solved this problem. We start with the basic techniques like Viola Jones face detector to some of the advanced techniques like Single Shot Detector. Some the the techniques are:</p>
<ul>
<li>Viola Jones face detector</li>
<li>Object Detection using Histogram of Oriented Gradients (HOG) Features</li>
<li>Scale-invariant feature transform (SIFT)</li>
<li>AlexNet</li>
<li>Region-based Convolutional Network (R-CNN)</li>
<li>You Only Look Once (YOLO)</li>
<li>Single Shot Detector (SSD)</li>
</ul>
<h2 id="what-is-object-detection">What is Object Detection?</h2>
<p>The formal definition for object detection is as follows:</p>
<blockquote>
<p>A Computer Vision technique to locate the presence of objects on images or videos. Object Detection comprises of two things i.e. Image Classification and Object Localization.</p>
</blockquote>
<p><strong>Image Classification</strong> answers the question &quot; What is in the picture/frame?&rdquo;. It takes an image and predicts the object in an image. For example, in the pictures below we can build a classifier that can detect a person in the picture and a bicycle.</p>

    <img src="https://i.imgur.com/rdfswAP.png"  alt="Object Detection in Working"  class="center"  style="border-radius: 8px;"  />


<p>But if both of them are in the same image then it becomes a problem. We could train a multilabel classifier but we still don’t know the positions of bicycle or person. The task of locating the object in the image is called <strong>Object localisation</strong>.</p>
<h2 id="why-we-need-object-detection">Why we need object detection?</h2>
<p>Object detection is a widely used technique in production systems. There are variants of object detection problem such as:</p>
<ul>
<li>Image classification</li>
<li>Image segmentation</li>
<li>Object detection has its own place and it is used as follows:</li>
</ul>
<p>An image has multiple objects but every application has a focus on a particular thing such as a face detection application is focused on finding a face, a traffic control system is focused on vechiles, an driving technology is focused on differentiating between vehicles and living beings. In the same line, Object detection technique helps to identify the image segment that the application needs to focus on.</p>
<p>It can be used to reduce the dimension of the image to only capture the object of interest and hence, improving the execution time greatly.</p>
<h2 id="object-detection-techniques">Object Detection Techniques</h2>
<p>Generally, Object detection is achieved by using either machine-learning based approaches or Deep learning based approaches.</p>
<h3 id="machine-learning-based-techniques">Machine Learning Based techniques</h3>
<p>In this approach, we define the features and then train the classifier (such as SVM) on the feature-set. Following are the machine learning based object detection techniques:</p>
<h4 id="1-viola-jones-face-detector-2001">1. Viola Jones face detector (2001)</h4>
<ul>
<li>It was the first efficient face detection algorithm to provide competitive results.</li>
<li>They hardcoded the features of the face (Haar Cascades) and then trained an SVM classifier on the featureset. Then they used that classifier to detect faces.</li>
<li>The downside of this algorithm was that is was unable to detect faces in other orientation or arrangement (such as wearing a mask, face tilted, etc.)</li>
</ul>
<h4 id="2-object-detection-using-histogram-of-oriented-gradients-hog-features">2. Object Detection using Histogram of Oriented Gradients (HOG) Features</h4>
<ul>
<li>Navneet Dalal and Bill Triggs introduced Histogram of Oriented Gradients(HOG) features in 2005.</li>
<li>The principle behind the histogram of oriented gradients descriptor is that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions.</li>
<li>The image is divided into small connected regions called cells, and for the pixels within each cell, a histogram of gradient directions is compiled. A descriptor is assigned to each detector window. This descriptor consists of all the cell histograms for each block in the detector window. The detector window descriptor is used as information for object recognition. Training and testing of classifiers such as SVM happens using this descriptor.</li>
</ul>

    <img src="https://i.imgur.com/MtIoUfK.png"  alt="HOG features"  class="center"  style="border-radius: 8px;"  />


<ul>
<li>Despite being good in many applications, it still used hand coded features which failed in a more generalized setting with much noise and distractions in the background.</li>
</ul>
<h4 id="3-scale-invariant-feature-transform-sift">3. Scale-invariant feature transform (SIFT)</h4>
<p>SIFT was created by David Lowe from the University British Columbia in 1999.The SIFT approach, for image feature generation, takes an image and transforms it into a large collection of local feature vectors. Each of these feature vectors is invariant to any scaling, rotation or translation of the image. There are four steps involved in the SIFT algorithm:</p>
<ul>
<li>Scale-space peak selection: Potential location for finding features.</li>
<li>Keypoint Localization: Accurately locating the feature keypoints.</li>
<li>Orientation Assignment: Assigning orientation to keypoints.</li>
<li>Keypoint descriptor: Describing the keypoints as a high dimensional vector.</li>
</ul>
<p>These resulting vectors are known as SIFT keys and are used in a nearest-neighbour approach to identify possible objects in an image.</p>

    <img src="https://i.imgur.com/DxRth6Z.png"  alt="SIFT features"  class="center"  style="border-radius: 8px;"  />


<h3 id="deep-learning-based-techniques">Deep Learning Based techniques</h3>
<p>Deep Learning techniques are able to do end-to-end object detection without specifically defining features, and are typically based on convolutional neural networks (CNN). A Convolutional Neural Network (CNN, or ConvNet) is a special kind of multi-layer neural networks, designed to recognize visual patterns directly from pixel images.</p>
<h4 id="1-alexnet">1. AlexNet</h4>
<p>In 2012, AlexNet significantly outperformed all prior competitors at ImageNet Large Scale Visual Recognition Challenge(ILSVRC) and won the challenge. Convolutional Neural Networks became the gold standard for image classification after Kriszhevsky&rsquo;s CNN&rsquo;s performance during ImageNet.</p>

    <img src="https://i.imgur.com/qIsJDE6.png"  alt="Alexnet"  class="center"  style="border-radius: 8px;"  />


<h4 id="2-region-based-convolutional-network-r-cnn">2. Region-based Convolutional Network (R-CNN)</h4>
<p>CNNs were too slow and computationally very expensive. R-CNN solves this problem by using an object proposal algorithm called Selective Search which reduces the number of bounding boxes that are fed to the classifier to close to 2000 region proposals.</p>
<p>In R-CNN, the selective search method developed by J.R.R. Uijlings and al. (2012) is an alternative to exhaustive search in an image to capture object location. It looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.</p>
<p>The main idea is composed of two steps. First, using selective search, it identifies a manageable number of bounding-box object region candidates (region of interest). And then it extracts CNN features from each region independently for classification.</p>

    <img src="https://i.imgur.com/aQ5a9MO.png"  alt="R-CNN"  class="center"  style="border-radius: 8px;"  />


<p>R-CNN was improved over the time for better performance. Fast Region-based Convolutional Network (Fast R-CNN) developed by R. Girshick (2015) reduced the time consumption related to the high number of models necessary to analyse all region proposals in R-CNN.</p>
<h4 id="3-you-only-look-once-yolo">3. You Only Look Once (YOLO)</h4>
<p>The YOLO model (J. Redmon et al., 2016) directly predicts bounding boxes and class probabilities with a single network in a single evaluation. They reframe the object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities.</p>
<p>YOLO divides each image into a grid of S x S and each grid predicts N bounding boxes and confidence. The confidence score tells us how certain it is that the predicted bounding box actually encloses some object.</p>

    <img src="https://i.imgur.com/09WoKLp.png"  alt="YOLO"  class="center"  style="border-radius: 8px;"  />


<p>Over time, it has become faster and better, with its versions named as: YOLO V1, YOLO V2 and YOLO V3. YOLO V2 is better than V1 in terms of accuracy and speed. YOLO V3 is more accurate than V2.</p>
<h4 id="4-single-shot-detectorssd">4. Single Shot Detector(SSD)</h4>
<p>SSD model was published (by Wei Liu et al.) in 2015, shortly after the YOLO model, and was also later refined in a subsequent paper.</p>
<p>Unlike YOLO, SSD does not split the image into grids of arbitrary size but predicts offset of predefined anchor boxes for every location of the feature map. Each box has a fixed size and position relative to its corresponding cell. All the anchor boxes tile the whole feature map in a convolutional manner.</p>
<p>Feature maps at different levels have different receptive field sizes. The anchor boxes on different levels are rescaled so that one feature map is only responsible for objects at one particular scale.</p>

    <img src="https://i.imgur.com/aCDTAje.png"  alt="YOLO"  class="center"  style="border-radius: 8px;"  />


]]></content>
        </item>
        
    </channel>
</rss>
